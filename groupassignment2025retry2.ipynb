{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanMuInCa/learn_python/blob/master/groupassignment2025retry2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkquzaHtsKA",
        "outputId": "59731998-b9ce-4c21-a567-da795ac1e483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of train data: (14994, 35)\n",
            "Shape after dropping missing values: (14993, 35)\n",
            "All force_meas values are positive.\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 14993 entries, 0 to 14992\n",
            "Data columns (total 35 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   tappingsteelgrade     14993 non-null  object \n",
            " 1   force_meas            14993 non-null  float64\n",
            " 2   speed                 14993 non-null  float64\n",
            " 3   entrytemperature      14993 non-null  float64\n",
            " 4   entrytemperaturebot   14993 non-null  float64\n",
            " 5   entrytemperaturecore  14993 non-null  float64\n",
            " 6   entrytemperaturetop   14993 non-null  float64\n",
            " 7   entrythickness        14993 non-null  float64\n",
            " 8   entrywidth            14993 non-null  float64\n",
            " 9   exitthickness         14993 non-null  float64\n",
            " 10  zeropoint             14993 non-null  float64\n",
            " 11  radius                14993 non-null  float64\n",
            " 12  pctal                 14993 non-null  float64\n",
            " 13  pctb                  14993 non-null  float64\n",
            " 14  pctc                  14993 non-null  float64\n",
            " 15  pctca                 14993 non-null  float64\n",
            " 16  pctco                 14993 non-null  float64\n",
            " 17  pctcr                 14993 non-null  float64\n",
            " 18  pctcu                 14993 non-null  float64\n",
            " 19  pcth                  14993 non-null  float64\n",
            " 20  pctmg                 14993 non-null  float64\n",
            " 21  pctmn                 14993 non-null  float64\n",
            " 22  pctmo                 14993 non-null  float64\n",
            " 23  pctn                  14993 non-null  float64\n",
            " 24  pctnb                 14993 non-null  float64\n",
            " 25  pctni                 14993 non-null  float64\n",
            " 26  pcto                  14993 non-null  float64\n",
            " 27  pctp                  14993 non-null  float64\n",
            " 28  pcts                  14993 non-null  float64\n",
            " 29  pctsi                 14993 non-null  float64\n",
            " 30  pctsn                 14993 non-null  float64\n",
            " 31  pctti                 14993 non-null  float64\n",
            " 32  pctv                  14993 non-null  float64\n",
            " 33  pctzr                 14993 non-null  float64\n",
            " 34  fur_line_no           14993 non-null  float64\n",
            "dtypes: float64(34), object(1)\n",
            "memory usage: 4.1+ MB\n",
            "None\n",
            "First 5 rows:\n",
            "  tappingsteelgrade   force_meas     speed  entrytemperature  \\\n",
            "0          GL4G71R1  58204143.19  2.634871       1000.661121   \n",
            "1          GL4G71R1  53211949.94  3.947747       1020.846126   \n",
            "2          JV7P1BP6  43513598.87  1.573130        829.078290   \n",
            "3          JT5P31P2  33293124.30  2.979649        736.655045   \n",
            "4          JT5P32P1  68174217.63  3.401038        893.722030   \n",
            "\n",
            "   entrytemperaturebot  entrytemperaturecore  entrytemperaturetop  \\\n",
            "0           939.942163           1034.327117           916.847574   \n",
            "1           991.621746           1044.907925           975.227384   \n",
            "2           786.696648            857.833946           763.533963   \n",
            "3           734.483511            738.987016           730.511453   \n",
            "4           867.859688            913.164933           850.840106   \n",
            "\n",
            "   entrythickness  entrywidth  exitthickness  ...  pctni    pcto    pctp  \\\n",
            "0        0.080232    3.496171       0.065490  ...  0.004  0.0000  0.0153   \n",
            "1        0.039173    3.974567       0.031835  ...  0.004  0.0000  0.0119   \n",
            "2        0.148507    1.912962       0.136898  ...  0.663  0.0025  0.0088   \n",
            "3        0.014806    2.715973       0.013670  ...  0.019  0.0017  0.0150   \n",
            "4        0.048040    3.654979       0.039451  ...  0.010  0.0019  0.0138   \n",
            "\n",
            "     pcts  pctsi  pctsn   pctti    pctv  pctzr  fur_line_no  \n",
            "0  0.0068  0.135  0.001  0.0097  0.0005  0.000          3.0  \n",
            "1  0.0051  0.128  0.001  0.0090  0.0005  0.000          4.0  \n",
            "2  0.0011  0.119  0.001  0.0121  0.0027  0.001          2.0  \n",
            "3  0.0031  0.283  0.002  0.0128  0.0019  0.001          2.0  \n",
            "4  0.0040  0.208  0.002  0.0135  0.0011  0.001          3.0  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (ensure 'train.csv' is in the working directory)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Print the initial shape of the dataset\n",
        "print(\"Initial shape of train data:\", df.shape)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df = df.dropna()\n",
        "print(\"Shape after dropping missing values:\", df.shape)\n",
        "\n",
        "# Check if any force_meas values are non-positive\n",
        "if (df['force_meas'] <= 0).any():\n",
        "    raise ValueError(\"There are non-positive values in force_meas!\")\n",
        "else:\n",
        "    print(\"All force_meas values are positive.\")\n",
        "\n",
        "# Display basic information and the first 5 rows of the dataset\n",
        "print(\"Dataset info:\")\n",
        "print(df.info())\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOpyctu_ulIB",
        "outputId": "77be77ca-96bb-4b99-d50a-581cf6bfbbaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape after encoding: (14993, 56)\n",
            "Feature scaling completed. Sample of scaled features:\n",
            "[[-1.20815045e+00  1.62317851e+00  1.10172575e+00  1.76238659e+00\n",
            "   9.87948550e-01  1.45623695e+00  3.61472968e-01  1.28464984e+00\n",
            "  -2.83133583e-01  1.73464961e+00 -8.51661554e-01 -1.95072152e+00\n",
            "   1.28237237e+00  2.64962246e-01  0.00000000e+00 -4.82318706e-01\n",
            "  -3.95594767e-01  3.58462510e-01  0.00000000e+00 -2.04614134e+00\n",
            "  -8.06154473e-01 -8.24254455e-01 -1.19837786e+00 -3.61909645e-01\n",
            "  -1.46499377e+00  1.43656069e+00  1.66689327e+00 -1.48847140e+00\n",
            "  -7.13871890e-01 -9.52200096e-01 -1.07879267e+00 -1.65922425e+00\n",
            "   4.48053053e-01 -4.08684453e-02  3.14958465e+00 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [ 1.81403894e-01  1.90906713e+00  1.88553755e+00  1.90154481e+00\n",
            "   1.90664373e+00 -6.79742133e-02  1.16145056e+00 -1.37449236e-01\n",
            "  -1.21715550e+00  2.67468694e-01 -4.56714531e-01 -1.95072152e+00\n",
            "   9.87407966e-01  8.76962173e-01  0.00000000e+00 -1.23416723e+00\n",
            "  -4.81487521e-01  2.18056736e+00  0.00000000e+00 -2.23225175e+00\n",
            "  -1.07843205e+00 -3.69278862e-01 -1.15916458e+00 -3.61909645e-01\n",
            "  -1.46499377e+00  1.40539080e-01  7.72697106e-01 -1.62638831e+00\n",
            "  -7.13871890e-01 -1.31325026e+00 -1.07879267e+00 -1.65922425e+00\n",
            "   1.34697710e+00 -4.08684453e-02  3.14958465e+00 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [-2.33190350e+00 -8.07020499e-01 -1.22251207e+00 -5.58842097e-01\n",
            "  -1.42467451e+00  3.99083949e+00 -2.28598226e+00  4.30203882e+00\n",
            "  -3.08520629e+00 -1.01109806e+00 -1.03502982e+00  1.01953925e+00\n",
            "  -2.16543376e+00  1.31410498e+00  0.00000000e+00  1.75620458e+01\n",
            "   1.34760850e+01 -9.43040956e-01  0.00000000e+00  1.32499496e+00\n",
            "   2.50602156e+01  3.89013792e-01  2.82751950e+00  2.20562425e+01\n",
            "   1.19251775e+00 -1.04112768e+00 -1.33129387e+00 -1.80371007e+00\n",
            "  -7.13871890e-01  2.85686174e-01  1.73109265e+00  6.70091258e-02\n",
            "  -4.50870997e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01  3.87078803e+01]\n",
            " [-8.43235919e-01 -2.11604937e+00 -2.01441616e+00 -2.12191053e+00\n",
            "  -1.94433398e+00 -9.72559432e-01 -9.43179965e-01 -9.05031891e-01\n",
            "   1.00114958e+00  8.06307865e-01 -2.16925267e-01  1.01953925e+00\n",
            "  -6.34896246e-01 -5.21894803e-01  0.00000000e+00  1.39730259e+00\n",
            "   7.68153799e-02  3.58462510e-01  0.00000000e+00  2.76009007e-01\n",
            "   1.06782643e-02 -2.17620332e-01  6.83859865e-01  1.48366958e-01\n",
            "   3.42114062e-01  1.32220584e+00 -2.79298382e-01  1.42748629e+00\n",
            "   2.73952867e-01  6.46736336e-01  7.09316169e-01  6.70091258e-02\n",
            "  -4.50870997e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01  2.89827535e+00\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [-3.97236250e-01  1.08555675e-01  8.46821474e-03  1.68868059e-01\n",
            "  -5.07794442e-02  2.61170485e-01  6.27032103e-01  1.84357643e-01\n",
            "  -1.66380670e-01 -1.80094545e+00  2.76758512e-01  2.94523289e-02\n",
            "  -1.38541678e+00 -8.71609047e-01  0.00000000e+00  1.44221726e-01\n",
            "  -2.23809259e-01 -6.82740263e-01  0.00000000e+00  4.74808764e-01\n",
            "   2.82955844e-01 -1.88586417e+00  4.35509054e-01 -1.57799004e-01\n",
            "   5.54714983e-01  8.64786451e-01  1.94099588e-01 -5.01949719e-02\n",
            "   2.73952867e-01  1.00778650e+00 -3.12460310e-01  6.70091258e-02\n",
            "   4.48053053e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01  3.54518346e+00 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]]\n"
          ]
        }
      ],
      "source": [
        "# Select features by dropping the target column 'force_meas'\n",
        "features = df.drop(columns=['force_meas'])\n",
        "target = df['force_meas']\n",
        "\n",
        "# Convert the categorical column 'tappingsteelgrade' using one-hot encoding\n",
        "if 'tappingsteelgrade' in features.columns:\n",
        "    features = pd.get_dummies(features, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "print(\"Features shape after encoding:\", features.shape)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "print(\"Feature scaling completed. Sample of scaled features:\")\n",
        "print(X_scaled[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMrZCqUFvIeQ",
        "outputId": "9e4e146f-37f6-49ed-a33b-b7ca1003e7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression CV MAE: 5565531.4078\n",
            "Decision Tree best params: {'max_depth': 10, 'min_samples_split': 10}\n",
            "Decision Tree CV MAE: 6599853.9875\n",
            "Random Forest best params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Random Forest CV MAE: 5252093.7735\n",
            "Gradient Boosting best params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "Gradient Boosting CV MAE: 4989130.7414\n",
            "Linear Regression Validation MAE: 5567526.7685\n",
            "Linear Regression Validation RMSE: 7214382.6856\n",
            "Decision Tree Validation MAE: 6520268.7094\n",
            "Decision Tree Validation RMSE: 8655189.8080\n",
            "Random Forest Validation MAE: 5127791.6150\n",
            "Random Forest Validation RMSE: 6684445.4882\n",
            "Gradient Boosting Validation MAE: 5006398.8995\n",
            "Gradient Boosting Validation RMSE: 6425564.8506\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Split the scaled features and target into training and validation sets\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, df['force_meas'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr_cv_scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
        "print(\"Linear Regression CV MAE: {:.4f}\".format(-np.mean(lr_cv_scores)))\n",
        "\n",
        "# -------------------------------\n",
        "# Decision Tree Regressor with Grid Search\n",
        "# -------------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "dt_grid = GridSearchCV(dt, dt_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Decision Tree CV MAE: {:.4f}\".format(-dt_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Random Forest Regressor with Grid Search\n",
        "# -------------------------------\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Random Forest CV MAE: {:.4f}\".format(-rf_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient Boosting Regressor with Grid Search\n",
        "# -------------------------------\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "gb_grid = GridSearchCV(gb, gb_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "print(\"Gradient Boosting best params:\", gb_grid.best_params_)\n",
        "print(\"Gradient Boosting CV MAE: {:.4f}\".format(-gb_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate each model on the validation set\n",
        "# -------------------------------\n",
        "models = {\n",
        "    'Linear Regression': lr,\n",
        "    'Decision Tree': dt_grid.best_estimator_,\n",
        "    'Random Forest': rf_grid.best_estimator_,\n",
        "    'Gradient Boosting': gb_grid.best_estimator_\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    print(f\"{name} Validation MAE: {mae:.4f}\")\n",
        "    print(f\"{name} Validation RMSE: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-y3N79LFJqH",
        "outputId": "bda72c2f-e4e9-47f6-e592-bb2427c2d7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training columns have been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "# Save the fitted scaler for later use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)  # Save the scaler object\n",
        "\n",
        "# Save the best Random Forest model (obtained from GridSearchCV)\n",
        "with open('best_rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_grid.best_estimator_, f)  # Save the best RF model\n",
        "\n",
        "# Save the best Gradient Boosting model (obtained from GridSearchCV)\n",
        "with open('best_gb_model.pkl', 'wb') as f:\n",
        "    pickle.dump(gb_grid.best_estimator_, f)  # Save the best GB model\n",
        "\n",
        "# Save the training columns (features names) for later use\n",
        "training_columns = features.columns.tolist()  # 'features' 是经过预处理后用于训练的 DataFrame\n",
        "with open('training_columns.pkl', 'wb') as f:\n",
        "    pickle.dump(training_columns, f)\n",
        "print(\"Training columns have been saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HIzk4uFPRDc",
        "outputId": "8b60cef0-2e7c-430a-ffa3-a2ed68e9979f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Metrics:\n",
            "Random Forest - MAE: 5055189.8127, RMSE: 6575285.0083, MSE: 43234372940812.3203, Avg Runtime per sample: 0.000040 sec\n",
            "Gradient Boosting - MAE: 4952478.7807, RMSE: 6351928.0018, MSE: 40346989340435.9062, Avg Runtime per sample: 0.000004 sec\n",
            "Baseline (force_pre) - MAE: 2898684.1133, RMSE: 3775600.3207, MSE: 14255157781834.8516\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Step 11: Final Evaluation on Test Data\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load test data (ensure test.csv is in your working directory)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Drop missing values if any (you may adjust based on your preprocessing strategy)\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# For test data, note: do not use the 'force_pre' column for prediction, it's the baseline.\n",
        "# We assume test_df contains 'force_pre' and other features including 'tappingsteelgrade'\n",
        "# Also assume that 'force_meas' is present for evaluation\n",
        "# If not, adjust accordingly (for final evaluation, force_meas is needed)\n",
        "\n",
        "# Separate baseline predictions for later comparison\n",
        "baseline_force_pre = test_df['force_pre']\n",
        "\n",
        "# Separate target variable for evaluation (force_meas)\n",
        "y_test = test_df['force_meas']\n",
        "\n",
        "# ===============================\n",
        "# Preprocessing: Feature Selection & Encoding\n",
        "# ===============================\n",
        "# Exclude the target 'force_meas' and baseline column 'force_pre'\n",
        "features_test = test_df.drop(columns=['force_meas', 'force_pre'])\n",
        "\n",
        "# One-hot encode the categorical column 'tappingsteelgrade' (using the same columns as during training)\n",
        "features_test = pd.get_dummies(features_test, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "# IMPORTANT: Align the test features with the training features.\n",
        "# Load the fitted scaler (assumes scaler.pkl was saved during training)\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# To ensure that test data has the same feature columns as training data,\n",
        "# we reindex the test DataFrame to match training columns.\n",
        "# Assume that 'training_columns' was saved during training.\n",
        "with open('training_columns.pkl', 'rb') as f:\n",
        "    training_columns = pickle.load(f)\n",
        "\n",
        "# Reindex the test features DataFrame; missing columns will be filled with zeros.\n",
        "features_test = features_test.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "# Scale the test features using the saved scaler\n",
        "X_test_scaled = scaler.transform(features_test)\n",
        "\n",
        "# ===============================\n",
        "# Load the saved models\n",
        "# ===============================\n",
        "# Load best Random Forest model\n",
        "with open('best_rf_model.pkl', 'rb') as f:\n",
        "    best_rf_model = pickle.load(f)\n",
        "\n",
        "# Load best Gradient Boosting model\n",
        "with open('best_gb_model.pkl', 'rb') as f:\n",
        "    best_gb_model = pickle.load(f)\n",
        "\n",
        "# ===============================\n",
        "# Evaluate Models on Test Data\n",
        "# ===============================\n",
        "# Evaluate Random Forest model\n",
        "start_time = time.time()\n",
        "rf_predictions = best_rf_model.predict(X_test_scaled)\n",
        "rf_runtime = (time.time() - start_time) / len(X_test_scaled)  # Average prediction time per sample\n",
        "\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Evaluate Gradient Boosting model\n",
        "start_time = time.time()\n",
        "gb_predictions = best_gb_model.predict(X_test_scaled)\n",
        "gb_runtime = (time.time() - start_time) / len(X_test_scaled)\n",
        "\n",
        "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
        "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
        "gb_mse = mean_squared_error(y_test, gb_predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"Random Forest - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    rf_mae, rf_rmse, rf_mse, rf_runtime))\n",
        "print(\"Gradient Boosting - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    gb_mae, gb_rmse, gb_mse, gb_runtime))\n",
        "\n",
        "# Compare with baseline force_pre (if available)\n",
        "# Calculate baseline evaluation metrics using force_pre column\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_force_pre)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_force_pre))\n",
        "baseline_mse = mean_squared_error(y_test, baseline_force_pre)\n",
        "\n",
        "print(\"Baseline (force_pre) - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}\".format(\n",
        "    baseline_mae, baseline_rmse, baseline_mse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEnNFrcQUqrb",
        "outputId": "2cf065ee-93b1-4e3b-80cc-8db059ad5107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 90 Complete [00h 01m 18s]\n",
            "val_mae: 50100368.0\n",
            "\n",
            "Best val_mae So Far: 43296880.0\n",
            "Total elapsed time: 00h 31m 35s\n",
            "Best hyperparameters found: {'num_layers': 2, 'units_0': 192, 'dropout_0': 0.1, 'l2_reg_0': 0.0009807310652433297, 'units_1': 224, 'dropout_1': 0.1, 'l2_reg_1': 1.164331764071064e-05, 'learning_rate': 0.009925530546395422, 'units_2': 224, 'dropout_2': 0.1, 'l2_reg_2': 0.000458502351256499, 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 1, 'tuner/round': 1, 'tuner/trial_id': '0076'}\n",
            "Epoch 1/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2701617107304448.0000 - mae: 50178972.0000 - val_loss: 2727466367975424.0000 - val_mae: 50450088.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2717974490251264.0000 - mae: 50307476.0000 - val_loss: 2725658723614720.0000 - val_mae: 50435756.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2721390868299776.0000 - mae: 50368904.0000 - val_loss: 2721965051740160.0000 - val_mae: 50404420.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2697291269931008.0000 - mae: 50137604.0000 - val_loss: 2719519067865088.0000 - val_mae: 50387344.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2676252909502464.0000 - mae: 49945724.0000 - val_loss: 2712267015585792.0000 - val_mae: 50326364.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2704877524353024.0000 - mae: 50211256.0000 - val_loss: 2708197802508288.0000 - val_mae: 50295000.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2677915330281472.0000 - mae: 50029100.0000 - val_loss: 2698049063223296.0000 - val_mae: 50209868.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2666467900260352.0000 - mae: 49904364.0000 - val_loss: 2691091753074688.0000 - val_mae: 50152716.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2691525813207040.0000 - mae: 50162140.0000 - val_loss: 2685713111842816.0000 - val_mae: 50113992.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2663707846901760.0000 - mae: 49870012.0000 - val_loss: 2677472680214528.0000 - val_mae: 50052408.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2668679539982336.0000 - mae: 49929852.0000 - val_loss: 2665686753083392.0000 - val_mae: 49958772.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2660760157159424.0000 - mae: 49903536.0000 - val_loss: 2656482638168064.0000 - val_mae: 49878416.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2631446736928768.0000 - mae: 49624792.0000 - val_loss: 2642252606210048.0000 - val_mae: 49752688.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2623679825444864.0000 - mae: 49558712.0000 - val_loss: 2631850195419136.0000 - val_mae: 49673296.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2587855503228928.0000 - mae: 49232060.0000 - val_loss: 2623777804386304.0000 - val_mae: 49611660.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2607435990695936.0000 - mae: 49448112.0000 - val_loss: 2603288931336192.0000 - val_mae: 49417552.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2583835413839872.0000 - mae: 49213936.0000 - val_loss: 2599941272764416.0000 - val_mae: 49412740.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2565755480571904.0000 - mae: 49047444.0000 - val_loss: 2577396989427712.0000 - val_mae: 49215496.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2568182673965056.0000 - mae: 49115396.0000 - val_loss: 2578338392571904.0000 - val_mae: 49234376.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2565925131780096.0000 - mae: 49112688.0000 - val_loss: 2559543347249152.0000 - val_mae: 49085284.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2539914272964608.0000 - mae: 48862220.0000 - val_loss: 2541404089745408.0000 - val_mae: 48928764.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2523814319620096.0000 - mae: 48757760.0000 - val_loss: 2535347917422592.0000 - val_mae: 48885892.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2516359061700608.0000 - mae: 48695368.0000 - val_loss: 2510048412565504.0000 - val_mae: 48657880.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2501875089801216.0000 - mae: 48553636.0000 - val_loss: 2482594444738560.0000 - val_mae: 48416532.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2482611893043200.0000 - mae: 48408860.0000 - val_loss: 2485580252315648.0000 - val_mae: 48454168.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2466497175748608.0000 - mae: 48250364.0000 - val_loss: 2486579369082880.0000 - val_mae: 48503560.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2448198199148544.0000 - mae: 48091828.0000 - val_loss: 2441765948751872.0000 - val_mae: 48071748.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2434659925360640.0000 - mae: 47983884.0000 - val_loss: 2447050100703232.0000 - val_mae: 48148096.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2418762372349952.0000 - mae: 47826288.0000 - val_loss: 2418074372276224.0000 - val_mae: 47870020.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2359675970387968.0000 - mae: 47270740.0000 - val_loss: 2392342686334976.0000 - val_mae: 47641220.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2387288046698496.0000 - mae: 47571276.0000 - val_loss: 2381238585262080.0000 - val_mae: 47552152.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2359092391706624.0000 - mae: 47308044.0000 - val_loss: 2358174342447104.0000 - val_mae: 47354588.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2320213542436864.0000 - mae: 46974036.0000 - val_loss: 2352828718776320.0000 - val_mae: 47320152.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2307082485235712.0000 - mae: 46835616.0000 - val_loss: 2337180810739712.0000 - val_mae: 47220036.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2290059919228928.0000 - mae: 46679788.0000 - val_loss: 2280527776186368.0000 - val_mae: 46630616.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2293321410019328.0000 - mae: 46737060.0000 - val_loss: 2256103102480384.0000 - val_mae: 46383548.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2254485778857984.0000 - mae: 46389828.0000 - val_loss: 2266699055235072.0000 - val_mae: 46520280.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2224948886110208.0000 - mae: 46133268.0000 - val_loss: 2228794358235136.0000 - val_mae: 46157092.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2196897112522752.0000 - mae: 45842808.0000 - val_loss: 2222763150409728.0000 - val_mae: 46144988.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 2176574266802176.0000 - mae: 45649372.0000 - val_loss: 2184486267650048.0000 - val_mae: 45734716.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2171851816042496.0000 - mae: 45623908.0000 - val_loss: 2182441728999424.0000 - val_mae: 45783288.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2153067373920256.0000 - mae: 45456956.0000 - val_loss: 2143897081872384.0000 - val_mae: 45396140.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2107453210624000.0000 - mae: 44974448.0000 - val_loss: 2129419015553024.0000 - val_mae: 45241932.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2092772878188544.0000 - mae: 44860944.0000 - val_loss: 2068316797534208.0000 - val_mae: 44589016.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2099505105207296.0000 - mae: 44942248.0000 - val_loss: 2080577050116096.0000 - val_mae: 44797336.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2081784875450368.0000 - mae: 44762528.0000 - val_loss: 2034367899631616.0000 - val_mae: 44269744.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2033556687683584.0000 - mae: 44281956.0000 - val_loss: 1992526999322624.0000 - val_mae: 43844844.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2011316810153984.0000 - mae: 44089560.0000 - val_loss: 1978550101999616.0000 - val_mae: 43693460.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1975826690080768.0000 - mae: 43646428.0000 - val_loss: 1985276155002880.0000 - val_mae: 43828844.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1945607031750656.0000 - mae: 43397456.0000 - val_loss: 1937205941501952.0000 - val_mae: 43293016.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized DNN model has been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install Keras Tuner if not already installed\n",
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "\n",
        "# Define a model-building function for the tuner\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Build a DNN model with hyperparameters to tune.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Input layer using an Input layer is preferred\n",
        "    model.add(keras.Input(shape=(X_train.shape[1],)))  # X_train为训练集特征，确保在运行前定义好\n",
        "\n",
        "    # Tune the number of layers (at least 1 layer)\n",
        "    num_layers = hp.Int('num_layers', 1, 3, default=2)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        # Tune the number of units in this layer\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32, default=64)\n",
        "        # Tune dropout rate for this layer\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1, default=0.2)\n",
        "        # Tune L2 regularization factor\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', 1e-5, 1e-3, sampling='LOG', default=1e-4)\n",
        "\n",
        "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate for the optimizer\n",
        "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Set up the tuner: Here we use Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='dnn_tuner_dir',\n",
        "    project_name='force_prediction_tuning'\n",
        ")\n",
        "\n",
        "# Optional: Early stopping callback to stop training if no improvement\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best hyperparameters and build the best model\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model and train it\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "history = best_model.fit(X_train, y_train,\n",
        "                         epochs=50,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[stop_early],\n",
        "                         verbose=1)\n",
        "\n",
        "\n",
        "best_model.save('optimized_dnn_model.h5')\n",
        "print(\"Optimized DNN model has been saved successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgxTtM1kf9A2",
        "outputId": "c006c484-f8ae-4919-cc34-f2687b6789dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from advanced_dnn_tuning/rolling_force_advanced/tuner0.json\n",
            "Best hyperparameters found: {'num_layers': 5, 'units_0': 256, 'activation_0': 'leaky_relu', 'dropout_0': 0.0, 'l2_reg_0': 2.5597162009390816e-05, 'batchnorm_0': True, 'units_1': 448, 'activation_1': 'relu', 'dropout_1': 0.2, 'l2_reg_1': 3.555769929080814e-06, 'batchnorm_1': False, 'units_2': 160, 'activation_2': 'leaky_relu', 'dropout_2': 0.30000000000000004, 'l2_reg_2': 8.068735178418917e-06, 'batchnorm_2': False, 'units_3': 384, 'activation_3': 'leaky_relu', 'dropout_3': 0.0, 'l2_reg_3': 0.0009514167995360483, 'batchnorm_3': True, 'learning_rate': 0.000582968539368285, 'units_4': 64, 'activation_4': 'relu', 'dropout_4': 0.2, 'l2_reg_4': 1.1015492294935071e-05, 'batchnorm_4': True, 'units_5': 352, 'activation_5': 'leaky_relu', 'dropout_5': 0.2, 'l2_reg_5': 1.760110534391205e-05, 'batchnorm_5': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 281.5579 - mae: 16.6825 - val_loss: 103.0264 - val_mae: 10.1177 - learning_rate: 5.8297e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 65.7511 - mae: 7.5580 - val_loss: 1.7259 - val_mae: 1.1360 - learning_rate: 5.8297e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 4.9721 - mae: 1.5615 - val_loss: 0.3162 - val_mae: 0.2441 - learning_rate: 5.8297e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 3.2626 - mae: 1.1531 - val_loss: 0.3056 - val_mae: 0.2308 - learning_rate: 5.8297e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 2.6963 - mae: 1.0162 - val_loss: 0.2782 - val_mae: 0.2154 - learning_rate: 5.8297e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.1915 - mae: 0.9053 - val_loss: 0.2508 - val_mae: 0.1885 - learning_rate: 5.8297e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 1.8558 - mae: 0.8167 - val_loss: 0.2343 - val_mae: 0.1785 - learning_rate: 5.8297e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 1.6412 - mae: 0.7417 - val_loss: 0.2285 - val_mae: 0.1844 - learning_rate: 5.8297e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 1.2315 - mae: 0.6371 - val_loss: 0.1990 - val_mae: 0.1576 - learning_rate: 5.8297e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 1.0402 - mae: 0.5675 - val_loss: 0.1847 - val_mae: 0.1534 - learning_rate: 5.8297e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.8222 - mae: 0.5011 - val_loss: 0.1854 - val_mae: 0.1770 - learning_rate: 5.8297e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.7089 - mae: 0.4605 - val_loss: 0.1564 - val_mae: 0.1505 - learning_rate: 5.8297e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.5985 - mae: 0.4174 - val_loss: 0.1447 - val_mae: 0.1538 - learning_rate: 5.8297e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.4617 - mae: 0.3609 - val_loss: 0.1255 - val_mae: 0.1407 - learning_rate: 5.8297e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.3674 - mae: 0.3179 - val_loss: 0.1241 - val_mae: 0.1614 - learning_rate: 5.8297e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.3118 - mae: 0.2858 - val_loss: 0.1016 - val_mae: 0.1421 - learning_rate: 5.8297e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.2650 - mae: 0.2663 - val_loss: 0.0893 - val_mae: 0.1388 - learning_rate: 5.8297e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1947 - mae: 0.2354 - val_loss: 0.0790 - val_mae: 0.1391 - learning_rate: 5.8297e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.1616 - mae: 0.2187 - val_loss: 0.0692 - val_mae: 0.1354 - learning_rate: 5.8297e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.1390 - mae: 0.2054 - val_loss: 0.0613 - val_mae: 0.1331 - learning_rate: 5.8297e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.1116 - mae: 0.1872 - val_loss: 0.0554 - val_mae: 0.1344 - learning_rate: 5.8297e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0855 - mae: 0.1726 - val_loss: 0.0488 - val_mae: 0.1304 - learning_rate: 5.8297e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0718 - mae: 0.1653 - val_loss: 0.0471 - val_mae: 0.1354 - learning_rate: 5.8297e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0601 - mae: 0.1541 - val_loss: 0.0407 - val_mae: 0.1288 - learning_rate: 5.8297e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0504 - mae: 0.1460 - val_loss: 0.0362 - val_mae: 0.1221 - learning_rate: 5.8297e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0449 - mae: 0.1413 - val_loss: 0.0339 - val_mae: 0.1192 - learning_rate: 5.8297e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0382 - mae: 0.1297 - val_loss: 0.0322 - val_mae: 0.1199 - learning_rate: 5.8297e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0354 - mae: 0.1282 - val_loss: 0.0303 - val_mae: 0.1162 - learning_rate: 5.8297e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0329 - mae: 0.1232 - val_loss: 0.0286 - val_mae: 0.1128 - learning_rate: 5.8297e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0303 - mae: 0.1181 - val_loss: 0.0282 - val_mae: 0.1141 - learning_rate: 5.8297e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0304 - mae: 0.1198 - val_loss: 0.0264 - val_mae: 0.1086 - learning_rate: 5.8297e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0273 - mae: 0.1121 - val_loss: 0.0237 - val_mae: 0.1002 - learning_rate: 5.8297e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0263 - mae: 0.1107 - val_loss: 0.0253 - val_mae: 0.1044 - learning_rate: 5.8297e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0252 - mae: 0.1075 - val_loss: 0.0238 - val_mae: 0.1022 - learning_rate: 5.8297e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0232 - mae: 0.1019 - val_loss: 0.0201 - val_mae: 0.0917 - learning_rate: 5.8297e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0220 - mae: 0.0995 - val_loss: 0.0195 - val_mae: 0.0902 - learning_rate: 5.8297e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0219 - mae: 0.0985 - val_loss: 0.0206 - val_mae: 0.0949 - learning_rate: 5.8297e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0212 - mae: 0.0977 - val_loss: 0.0193 - val_mae: 0.0899 - learning_rate: 5.8297e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0206 - mae: 0.0964 - val_loss: 0.0159 - val_mae: 0.0788 - learning_rate: 5.8297e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0187 - mae: 0.0909 - val_loss: 0.0166 - val_mae: 0.0804 - learning_rate: 5.8297e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0196 - mae: 0.0949 - val_loss: 0.0160 - val_mae: 0.0803 - learning_rate: 5.8297e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0181 - mae: 0.0913 - val_loss: 0.0148 - val_mae: 0.0785 - learning_rate: 5.8297e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0176 - mae: 0.0897 - val_loss: 0.0147 - val_mae: 0.0791 - learning_rate: 5.8297e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0177 - mae: 0.0901 - val_loss: 0.0180 - val_mae: 0.0920 - learning_rate: 5.8297e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0167 - mae: 0.0880 - val_loss: 0.0151 - val_mae: 0.0824 - learning_rate: 5.8297e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0163 - mae: 0.0869 - val_loss: 0.0158 - val_mae: 0.0872 - learning_rate: 5.8297e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0155 - mae: 0.0843 - val_loss: 0.0134 - val_mae: 0.0742 - learning_rate: 5.8297e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0158 - mae: 0.0864 - val_loss: 0.0133 - val_mae: 0.0774 - learning_rate: 5.8297e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0157 - mae: 0.0860 - val_loss: 0.0130 - val_mae: 0.0729 - learning_rate: 5.8297e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0158 - mae: 0.0870 - val_loss: 0.0188 - val_mae: 0.1021 - learning_rate: 5.8297e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0158 - mae: 0.0880 - val_loss: 0.0129 - val_mae: 0.0768 - learning_rate: 5.8297e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0142 - mae: 0.0815 - val_loss: 0.0115 - val_mae: 0.0688 - learning_rate: 5.8297e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0145 - mae: 0.0824 - val_loss: 0.0106 - val_mae: 0.0656 - learning_rate: 5.8297e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0155 - mae: 0.0876 - val_loss: 0.0157 - val_mae: 0.0869 - learning_rate: 5.8297e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0146 - mae: 0.0846 - val_loss: 0.0126 - val_mae: 0.0753 - learning_rate: 5.8297e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0135 - mae: 0.0799 - val_loss: 0.0139 - val_mae: 0.0797 - learning_rate: 5.8297e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0148 - mae: 0.0849 - val_loss: 0.0217 - val_mae: 0.0989 - learning_rate: 5.8297e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m373/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0134 - mae: 0.0797\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00029148426256142557.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0134 - mae: 0.0797 - val_loss: 0.0131 - val_mae: 0.0765 - learning_rate: 5.8297e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0123 - mae: 0.0761 - val_loss: 0.0109 - val_mae: 0.0672 - learning_rate: 2.9148e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0113 - mae: 0.0730 - val_loss: 0.0086 - val_mae: 0.0578 - learning_rate: 2.9148e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0117 - mae: 0.0747 - val_loss: 0.0091 - val_mae: 0.0599 - learning_rate: 2.9148e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0108 - mae: 0.0714 - val_loss: 0.0086 - val_mae: 0.0599 - learning_rate: 2.9148e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - loss: 0.0116 - mae: 0.0750 - val_loss: 0.0091 - val_mae: 0.0630 - learning_rate: 2.9148e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0109 - mae: 0.0718 - val_loss: 0.0084 - val_mae: 0.0582 - learning_rate: 2.9148e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0114 - mae: 0.0735 - val_loss: 0.0080 - val_mae: 0.0563 - learning_rate: 2.9148e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0107 - mae: 0.0705 - val_loss: 0.0087 - val_mae: 0.0603 - learning_rate: 2.9148e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0106 - mae: 0.0712 - val_loss: 0.0081 - val_mae: 0.0575 - learning_rate: 2.9148e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0109 - mae: 0.0718 - val_loss: 0.0087 - val_mae: 0.0605 - learning_rate: 2.9148e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0102 - mae: 0.0690 - val_loss: 0.0086 - val_mae: 0.0609 - learning_rate: 2.9148e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0105 - mae: 0.0708 - val_loss: 0.0078 - val_mae: 0.0569 - learning_rate: 2.9148e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0110 - mae: 0.0728 - val_loss: 0.0076 - val_mae: 0.0552 - learning_rate: 2.9148e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0106 - mae: 0.0716 - val_loss: 0.0090 - val_mae: 0.0639 - learning_rate: 2.9148e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - loss: 0.0105 - mae: 0.0713 - val_loss: 0.0076 - val_mae: 0.0551 - learning_rate: 2.9148e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0105 - mae: 0.0711 - val_loss: 0.0091 - val_mae: 0.0623 - learning_rate: 2.9148e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - loss: 0.0102 - mae: 0.0706 - val_loss: 0.0078 - val_mae: 0.0562 - learning_rate: 2.9148e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m371/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0110 - mae: 0.0736\n",
            "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.00014574213128071278.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - loss: 0.0110 - mae: 0.0736 - val_loss: 0.0085 - val_mae: 0.0597 - learning_rate: 2.9148e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - loss: 0.0095 - mae: 0.0674 - val_loss: 0.0073 - val_mae: 0.0557 - learning_rate: 1.4574e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0088 - mae: 0.0642 - val_loss: 0.0063 - val_mae: 0.0483 - learning_rate: 1.4574e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0089 - mae: 0.0646 - val_loss: 0.0067 - val_mae: 0.0516 - learning_rate: 1.4574e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0093 - mae: 0.0667 - val_loss: 0.0066 - val_mae: 0.0512 - learning_rate: 1.4574e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0091 - mae: 0.0657 - val_loss: 0.0069 - val_mae: 0.0530 - learning_rate: 1.4574e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0092 - mae: 0.0670 - val_loss: 0.0069 - val_mae: 0.0532 - learning_rate: 1.4574e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m374/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0089 - mae: 0.0651\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 7.287106564035639e-05.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0089 - mae: 0.0651 - val_loss: 0.0069 - val_mae: 0.0532 - learning_rate: 1.4574e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0088 - mae: 0.0656 - val_loss: 0.0059 - val_mae: 0.0475 - learning_rate: 7.2871e-05\n",
            "Epoch 85/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0080 - mae: 0.0603 - val_loss: 0.0063 - val_mae: 0.0502 - learning_rate: 7.2871e-05\n",
            "Epoch 86/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0083 - mae: 0.0633 - val_loss: 0.0061 - val_mae: 0.0484 - learning_rate: 7.2871e-05\n",
            "Epoch 87/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - loss: 0.0087 - mae: 0.0655 - val_loss: 0.0061 - val_mae: 0.0483 - learning_rate: 7.2871e-05\n",
            "Epoch 88/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0083 - mae: 0.0630 - val_loss: 0.0061 - val_mae: 0.0488 - learning_rate: 7.2871e-05\n",
            "Epoch 89/100\n",
            "\u001b[1m374/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0084 - mae: 0.0640\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 3.6435532820178196e-05.\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0084 - mae: 0.0640 - val_loss: 0.0059 - val_mae: 0.0470 - learning_rate: 7.2871e-05\n",
            "Epoch 90/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0080 - mae: 0.0618 - val_loss: 0.0060 - val_mae: 0.0489 - learning_rate: 3.6436e-05\n",
            "Epoch 91/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0084 - mae: 0.0640 - val_loss: 0.0057 - val_mae: 0.0459 - learning_rate: 3.6436e-05\n",
            "Epoch 92/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0078 - mae: 0.0600 - val_loss: 0.0057 - val_mae: 0.0468 - learning_rate: 3.6436e-05\n",
            "Epoch 93/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 0.0084 - mae: 0.0635 - val_loss: 0.0057 - val_mae: 0.0463 - learning_rate: 3.6436e-05\n",
            "Epoch 94/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0081 - mae: 0.0619 - val_loss: 0.0058 - val_mae: 0.0468 - learning_rate: 3.6436e-05\n",
            "Epoch 95/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0078 - mae: 0.0607 - val_loss: 0.0057 - val_mae: 0.0462 - learning_rate: 3.6436e-05\n",
            "Epoch 96/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 11ms/step - loss: 0.0082 - mae: 0.0631 - val_loss: 0.0056 - val_mae: 0.0457 - learning_rate: 3.6436e-05\n",
            "Epoch 97/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.0080 - mae: 0.0618 - val_loss: 0.0055 - val_mae: 0.0455 - learning_rate: 3.6436e-05\n",
            "Epoch 98/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - loss: 0.0079 - mae: 0.0619 - val_loss: 0.0057 - val_mae: 0.0470 - learning_rate: 3.6436e-05\n",
            "Epoch 99/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.0079 - mae: 0.0615 - val_loss: 0.0057 - val_mae: 0.0463 - learning_rate: 3.6436e-05\n",
            "Epoch 100/100\n",
            "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - loss: 0.0080 - mae: 0.0628 - val_loss: 0.0058 - val_mae: 0.0468 - learning_rate: 3.6436e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m94/94\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "Advanced Optimized DNN (with log transformation) Validation MAE: 2182302.7720\n",
            "Advanced Optimized DNN (with log transformation) Validation RMSE: 2923777.2524\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "scaler_dnn_log = StandardScaler()\n",
        "X_train_scaled = scaler_dnn_log.fit_transform(X_train)\n",
        "X_val_scaled   = scaler_dnn_log.transform(X_val)\n",
        "# scaler\n",
        "with open('scaler_dnn_log.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_dnn_log, f)\n",
        "\n",
        "\n",
        "# Assume that X_train, y_train, X_val, and y_val are already preprocessed\n",
        "# and that y_train and y_val have been log-transformed\n",
        "y_train_log = np.log(y_train)\n",
        "y_val_log = np.log(y_val)\n",
        "\n",
        "# Get the number of features from X_train\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Define the model-building function for the tuner\n",
        "def build_advanced_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "    # Loop to add multiple hidden layers (between 4 and 6 layers)\n",
        "    for i in range(hp.Int('num_layers', 4, 6)):\n",
        "        # Define the number of units in the layer (range: 32 to 512, step size: 32)\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=512, step=32)\n",
        "        # Choose the activation function: ReLU or LeakyReLU\n",
        "        activation_choice = hp.Choice(f'activation_{i}', values=['relu', 'leaky_relu'])\n",
        "        # Define dropout rate (from 0.0 to 0.5)\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Define L2 regularization factor (using log sampling)\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "        # Add a Dense layer with L2 regularization\n",
        "        model.add(layers.Dense(units, kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "\n",
        "        # Add the chosen activation function\n",
        "        if activation_choice == 'leaky_relu':\n",
        "            model.add(layers.LeakyReLU(alpha=0.1))\n",
        "        else:\n",
        "            model.add(layers.Activation('relu'))\n",
        "\n",
        "        # Add a Dropout layer\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "        # Optionally add Batch Normalization after each layer\n",
        "        if hp.Boolean(f'batchnorm_{i}'):\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Output layer for predicting log(force_meas)\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Define the learning rate\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model with MSE loss and MAE metric\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Use Keras Tuner for hyperparameter tuning with RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_advanced_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=30,\n",
        "    executions_per_trial=1,\n",
        "    directory='advanced_dnn_tuning',\n",
        "    project_name='rolling_force_advanced'\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search(X_train, y_train_log, epochs=50, validation_data=(X_val, y_val_log))\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model using the best hyperparameters\n",
        "best_advanced_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "# Define callbacks for learning rate reduction and early stopping\n",
        "lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the best model\n",
        "history = best_advanced_model.fit(X_train, y_train_log,\n",
        "                                  epochs=100,\n",
        "                                  validation_data=(X_val, y_val_log),\n",
        "                                  callbacks=[lr_reducer, early_stopper])\n",
        "\n",
        "# Save the optimized model (using HDF5 format; you can also use the native Keras format)\n",
        "best_advanced_model.save(\"optimized_dnn_model_advanced_log.h5\")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate the optimized model on the validation set\n",
        "# -------------------------------\n",
        "# Predict log-transformed values on validation set and revert the transformation\n",
        "y_val_pred_log = best_advanced_model.predict(X_val).flatten()\n",
        "y_val_pred = np.exp(y_val_pred_log)  # Reverse log transformation\n",
        "y_val_orig = np.exp(y_val_log)\n",
        "\n",
        "# Calculate evaluation metrics on the original scale\n",
        "val_mae = mean_absolute_error(y_val_orig, y_val_pred)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_orig, y_val_pred))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation MAE: {:.4f}\".format(val_mae))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation RMSE: {:.4f}\".format(val_rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "1) Load train/test\n",
        "2) Preprocess (one-hot, align columns, scale)\n",
        "3) Log-transform y\n",
        "4) Tune + train on scaled data\n",
        "5) Final evaluate on test.csv\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load data\n",
        "# ----------------------------\n",
        "df = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Split train/val\n",
        "# ----------------------------\n",
        "X = df.drop(columns=['force_meas'])\n",
        "y = df['force_meas'].values\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare test\n",
        "X_test = df_test.drop(columns=['force_meas','force_pre'])\n",
        "y_test = df_test['force_meas'].values\n",
        "baseline_pre = df_test['force_pre'].values\n",
        "\n",
        "# ----------------------------\n",
        "# 3. One-hot encode\n",
        "# ----------------------------\n",
        "def onehot(df_in):\n",
        "    return pd.get_dummies(df_in, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "X_train = onehot(X_train)\n",
        "X_val   = onehot(X_val)\n",
        "X_test  = onehot(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Align columns\n",
        "# ----------------------------\n",
        "cols = X_train.columns.tolist()\n",
        "with open('training_columns.pkl','wb') as f:\n",
        "    pickle.dump(cols, f)\n",
        "\n",
        "X_val   = X_val.reindex(columns=cols, fill_value=0)\n",
        "X_test  = X_test.reindex(columns=cols, fill_value=0)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Scale features\n",
        "# ----------------------------\n",
        "scaler = StandardScaler().fit(X_train.values)\n",
        "X_train_s = scaler.transform(X_train.values)\n",
        "X_val_s   = scaler.transform(X_val.values)\n",
        "X_test_s  = scaler.transform(X_test.values)\n",
        "\n",
        "with open('scaler_dnn_log.pkl','wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Log-transform y\n",
        "# ----------------------------\n",
        "y_train_log = np.log(y_train)\n",
        "y_val_log   = np.log(y_val)\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Build tuner\n",
        "# ----------------------------\n",
        "input_dim = X_train_s.shape[1]\n",
        "\n",
        "def build_model(hp):\n",
        "    m = keras.Sequential([layers.Input(shape=(input_dim,))])\n",
        "    for i in range(hp.Int('num_layers',4,6)):\n",
        "        units = hp.Int(f'units_{i}',32,512,32)\n",
        "        l2r   = hp.Float(f'l2_{i}',1e-6,1e-3,sampling='LOG')\n",
        "        m.add(layers.Dense(units, kernel_regularizer=regularizers.l2(l2r)))\n",
        "        if hp.Choice(f'act_{i}', ['relu','leaky_relu'])=='leaky_relu':\n",
        "            m.add(layers.LeakyReLU())\n",
        "        else:\n",
        "            m.add(layers.Activation('relu'))\n",
        "        m.add(layers.Dropout(hp.Float(f'drop_{i}',0,0.5,0.1)))\n",
        "        if hp.Boolean(f'bn_{i}'):\n",
        "            m.add(layers.BatchNormalization())\n",
        "    m.add(layers.Dense(1))\n",
        "    lr = hp.Float('lr',1e-4,1e-2,sampling='LOG')\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss='mse', metrics=['mae'])\n",
        "    return m\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model, objective='val_mae',\n",
        "    max_trials=30, executions_per_trial=1,\n",
        "    directory='advanced_dnn_tuning', project_name='rolling_force'\n",
        ")\n",
        "tuner.search(X_train_s, y_train_log,\n",
        "             validation_data=(X_val_s, y_val_log),\n",
        "             epochs=50,\n",
        "             callbacks=[keras.callbacks.EarlyStopping('val_mae',patience=5)])\n",
        "\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"Best HP:\", best_hp.values)\n",
        "\n",
        "# ----------------------------\n",
        "# 8. Train best model\n",
        "# ----------------------------\n",
        "model = tuner.hypermodel.build(best_hp)\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau('val_loss',factor=0.5,patience=5,verbose=1),\n",
        "    keras.callbacks.EarlyStopping('val_loss',patience=10,restore_best_weights=True)\n",
        "]\n",
        "model.fit(X_train_s, y_train_log,\n",
        "          validation_data=(X_val_s,y_val_log),\n",
        "          epochs=100, batch_size=256,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.save('optimized_dnn_model_advanced_log.h5')\n",
        "\n",
        "# ----------------------------\n",
        "# 9. Final evaluation on test.csv\n",
        "# ----------------------------\n",
        "start = time.time()\n",
        "y_pred_log = model.predict(X_test_s).flatten()\n",
        "runtime = (time.time()-start)/len(X_test_s)\n",
        "\n",
        "y_pred = np.exp(y_pred_log)\n",
        "\n",
        "mae  = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mse  = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"DNN Test MAE: {mae:.4f}, RMSE: {rmse:.4f}, MSE: {mse:.4f}, time/sample: {runtime:.6f}s\")\n",
        "\n",
        "# baseline\n",
        "b_mae  = mean_absolute_error(y_test, baseline_pre)\n",
        "b_rmse = np.sqrt(mean_squared_error(y_test, baseline_pre))\n",
        "b_mse  = mean_squared_error(y_test, baseline_pre)\n",
        "print(f\"Baseline MAE: {b_mae:.4f}, RMSE: {b_rmse:.4f}, MSE: {b_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASFTm2TH3qkF",
        "outputId": "e668a477-61a0-440c-e89c-3314c31e8143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 11m 18s]\n",
            "val_mae: 0.07284444570541382\n",
            "\n",
            "Best val_mae So Far: 0.05394996702671051\n",
            "Total elapsed time: 03h 26m 06s\n",
            "Best HP: {'num_layers': 4, 'units_0': 64, 'l2_0': 2.1646574771386216e-06, 'act_0': 'leaky_relu', 'drop_0': 0.1, 'bn_0': False, 'units_1': 128, 'l2_1': 3.997743842873214e-05, 'act_1': 'relu', 'drop_1': 0.0, 'bn_1': False, 'units_2': 480, 'l2_2': 0.0004621378949210946, 'act_2': 'relu', 'drop_2': 0.2, 'bn_2': True, 'units_3': 192, 'l2_3': 2.1648531537582487e-06, 'act_3': 'leaky_relu', 'drop_3': 0.1, 'bn_3': True, 'lr': 0.0007459622916446641, 'units_4': 192, 'l2_4': 0.00010475693111475635, 'act_4': 'relu', 'drop_4': 0.2, 'bn_4': True, 'units_5': 256, 'l2_5': 3.714400064260753e-06, 'act_5': 'relu', 'drop_5': 0.30000000000000004, 'bn_5': True}\n",
            "Epoch 1/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - loss: 272.5034 - mae: 16.3876 - val_loss: 67.3738 - val_mae: 8.1851 - learning_rate: 7.4596e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 36.5416 - mae: 5.4086 - val_loss: 0.8070 - val_mae: 0.5282 - learning_rate: 7.4596e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 2.6827 - mae: 1.1244 - val_loss: 0.2664 - val_mae: 0.2584 - learning_rate: 7.4596e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 2.2902 - mae: 1.0018 - val_loss: 0.4812 - val_mae: 0.3202 - learning_rate: 7.4596e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 2.0497 - mae: 0.9211 - val_loss: 0.1652 - val_mae: 0.1953 - learning_rate: 7.4596e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.8781 - mae: 0.8574 - val_loss: 0.2229 - val_mae: 0.2655 - learning_rate: 7.4596e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 1.6560 - mae: 0.7906 - val_loss: 0.1520 - val_mae: 0.1829 - learning_rate: 7.4596e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 1.4600 - mae: 0.7426 - val_loss: 0.1898 - val_mae: 0.2153 - learning_rate: 7.4596e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 1.3785 - mae: 0.7110 - val_loss: 0.1385 - val_mae: 0.1847 - learning_rate: 7.4596e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 1.2003 - mae: 0.6624 - val_loss: 0.1194 - val_mae: 0.1501 - learning_rate: 7.4596e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 1.0851 - mae: 0.6297 - val_loss: 0.1200 - val_mae: 0.1677 - learning_rate: 7.4596e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.9738 - mae: 0.5996 - val_loss: 0.1138 - val_mae: 0.1600 - learning_rate: 7.4596e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.8579 - mae: 0.5624 - val_loss: 0.1223 - val_mae: 0.1845 - learning_rate: 7.4596e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.8095 - mae: 0.5373 - val_loss: 0.1019 - val_mae: 0.1528 - learning_rate: 7.4596e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.7228 - mae: 0.5101 - val_loss: 0.0916 - val_mae: 0.1394 - learning_rate: 7.4596e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.6373 - mae: 0.4819 - val_loss: 0.0949 - val_mae: 0.1549 - learning_rate: 7.4596e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.5806 - mae: 0.4552 - val_loss: 0.0848 - val_mae: 0.1409 - learning_rate: 7.4596e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.5240 - mae: 0.4332 - val_loss: 0.0872 - val_mae: 0.1498 - learning_rate: 7.4596e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.4792 - mae: 0.4108 - val_loss: 0.0757 - val_mae: 0.1389 - learning_rate: 7.4596e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.4072 - mae: 0.3837 - val_loss: 0.0842 - val_mae: 0.1632 - learning_rate: 7.4596e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.3750 - mae: 0.3647 - val_loss: 0.0677 - val_mae: 0.1372 - learning_rate: 7.4596e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.3472 - mae: 0.3497 - val_loss: 0.0712 - val_mae: 0.1480 - learning_rate: 7.4596e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.2902 - mae: 0.3251 - val_loss: 0.0626 - val_mae: 0.1416 - learning_rate: 7.4596e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.2590 - mae: 0.3040 - val_loss: 0.0561 - val_mae: 0.1326 - learning_rate: 7.4596e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.2309 - mae: 0.2883 - val_loss: 0.0516 - val_mae: 0.1297 - learning_rate: 7.4596e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.1954 - mae: 0.2678 - val_loss: 0.0479 - val_mae: 0.1269 - learning_rate: 7.4596e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.1820 - mae: 0.2561 - val_loss: 0.0458 - val_mae: 0.1280 - learning_rate: 7.4596e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.1560 - mae: 0.2410 - val_loss: 0.0430 - val_mae: 0.1258 - learning_rate: 7.4596e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.1387 - mae: 0.2281 - val_loss: 0.0396 - val_mae: 0.1215 - learning_rate: 7.4596e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.1205 - mae: 0.2155 - val_loss: 0.0414 - val_mae: 0.1317 - learning_rate: 7.4596e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.1060 - mae: 0.2036 - val_loss: 0.0390 - val_mae: 0.1288 - learning_rate: 7.4596e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0961 - mae: 0.1946 - val_loss: 0.0351 - val_mae: 0.1226 - learning_rate: 7.4596e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0837 - mae: 0.1835 - val_loss: 0.0355 - val_mae: 0.1266 - learning_rate: 7.4596e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0766 - mae: 0.1767 - val_loss: 0.0332 - val_mae: 0.1215 - learning_rate: 7.4596e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0663 - mae: 0.1664 - val_loss: 0.0311 - val_mae: 0.1192 - learning_rate: 7.4596e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0594 - mae: 0.1599 - val_loss: 0.0294 - val_mae: 0.1165 - learning_rate: 7.4596e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0517 - mae: 0.1511 - val_loss: 0.0289 - val_mae: 0.1169 - learning_rate: 7.4596e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0504 - mae: 0.1497 - val_loss: 0.0305 - val_mae: 0.1220 - learning_rate: 7.4596e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 0.0461 - mae: 0.1446 - val_loss: 0.0265 - val_mae: 0.1120 - learning_rate: 7.4596e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0405 - mae: 0.1381 - val_loss: 0.0259 - val_mae: 0.1116 - learning_rate: 7.4596e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0386 - mae: 0.1346 - val_loss: 0.0283 - val_mae: 0.1190 - learning_rate: 7.4596e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0353 - mae: 0.1302 - val_loss: 0.0244 - val_mae: 0.1077 - learning_rate: 7.4596e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0332 - mae: 0.1261 - val_loss: 0.0229 - val_mae: 0.1040 - learning_rate: 7.4596e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0304 - mae: 0.1209 - val_loss: 0.0225 - val_mae: 0.1031 - learning_rate: 7.4596e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0276 - mae: 0.1140 - val_loss: 0.0183 - val_mae: 0.0897 - learning_rate: 7.4596e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0246 - mae: 0.1073 - val_loss: 0.0159 - val_mae: 0.0821 - learning_rate: 7.4596e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0228 - mae: 0.1031 - val_loss: 0.0143 - val_mae: 0.0774 - learning_rate: 7.4596e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0206 - mae: 0.0966 - val_loss: 0.0139 - val_mae: 0.0762 - learning_rate: 7.4596e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0186 - mae: 0.0918 - val_loss: 0.0133 - val_mae: 0.0734 - learning_rate: 7.4596e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0170 - mae: 0.0867 - val_loss: 0.0120 - val_mae: 0.0673 - learning_rate: 7.4596e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0161 - mae: 0.0836 - val_loss: 0.0121 - val_mae: 0.0691 - learning_rate: 7.4596e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.0149 - mae: 0.0804 - val_loss: 0.0116 - val_mae: 0.0704 - learning_rate: 7.4596e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0142 - mae: 0.0784 - val_loss: 0.0093 - val_mae: 0.0573 - learning_rate: 7.4596e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0132 - mae: 0.0753 - val_loss: 0.0100 - val_mae: 0.0616 - learning_rate: 7.4596e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0128 - mae: 0.0747 - val_loss: 0.0091 - val_mae: 0.0576 - learning_rate: 7.4596e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0121 - mae: 0.0723 - val_loss: 0.0088 - val_mae: 0.0579 - learning_rate: 7.4596e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0118 - mae: 0.0720 - val_loss: 0.0114 - val_mae: 0.0726 - learning_rate: 7.4596e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0119 - mae: 0.0729 - val_loss: 0.0080 - val_mae: 0.0559 - learning_rate: 7.4596e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0115 - mae: 0.0720 - val_loss: 0.0075 - val_mae: 0.0510 - learning_rate: 7.4596e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step - loss: 0.0104 - mae: 0.0673 - val_loss: 0.0091 - val_mae: 0.0626 - learning_rate: 7.4596e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0102 - mae: 0.0673 - val_loss: 0.0088 - val_mae: 0.0624 - learning_rate: 7.4596e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0104 - mae: 0.0694 - val_loss: 0.0075 - val_mae: 0.0551 - learning_rate: 7.4596e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - loss: 0.0096 - mae: 0.0654 - val_loss: 0.0070 - val_mae: 0.0511 - learning_rate: 7.4596e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0100 - mae: 0.0670 - val_loss: 0.0079 - val_mae: 0.0562 - learning_rate: 7.4596e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0095 - mae: 0.0657 - val_loss: 0.0081 - val_mae: 0.0581 - learning_rate: 7.4596e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0094 - mae: 0.0659 - val_loss: 0.0065 - val_mae: 0.0503 - learning_rate: 7.4596e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0091 - mae: 0.0635 - val_loss: 0.0065 - val_mae: 0.0493 - learning_rate: 7.4596e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0090 - mae: 0.0640 - val_loss: 0.0078 - val_mae: 0.0586 - learning_rate: 7.4596e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0088 - mae: 0.0628 - val_loss: 0.0066 - val_mae: 0.0520 - learning_rate: 7.4596e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0085 - mae: 0.0629 - val_loss: 0.0083 - val_mae: 0.0629 - learning_rate: 7.4596e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0086 - mae: 0.0633 - val_loss: 0.0061 - val_mae: 0.0489 - learning_rate: 7.4596e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0088 - mae: 0.0641 - val_loss: 0.0060 - val_mae: 0.0501 - learning_rate: 7.4596e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0083 - mae: 0.0623 - val_loss: 0.0057 - val_mae: 0.0478 - learning_rate: 7.4596e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0081 - mae: 0.0618 - val_loss: 0.0074 - val_mae: 0.0598 - learning_rate: 7.4596e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0080 - mae: 0.0613 - val_loss: 0.0073 - val_mae: 0.0583 - learning_rate: 7.4596e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.0618 - val_loss: 0.0066 - val_mae: 0.0548 - learning_rate: 7.4596e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0079 - mae: 0.0615 - val_loss: 0.0077 - val_mae: 0.0587 - learning_rate: 7.4596e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0078 - mae: 0.0608\n",
            "Epoch 78: ReduceLROnPlateau reducing learning rate to 0.00037298115785233676.\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0078 - mae: 0.0608 - val_loss: 0.0064 - val_mae: 0.0521 - learning_rate: 7.4596e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0069 - mae: 0.0572 - val_loss: 0.0059 - val_mae: 0.0503 - learning_rate: 3.7298e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0067 - mae: 0.0559 - val_loss: 0.0056 - val_mae: 0.0492 - learning_rate: 3.7298e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 0.0069 - mae: 0.0574 - val_loss: 0.0074 - val_mae: 0.0611 - learning_rate: 3.7298e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0066 - mae: 0.0563 - val_loss: 0.0049 - val_mae: 0.0449 - learning_rate: 3.7298e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0067 - mae: 0.0563 - val_loss: 0.0052 - val_mae: 0.0473 - learning_rate: 3.7298e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.0069 - mae: 0.0584 - val_loss: 0.0051 - val_mae: 0.0474 - learning_rate: 3.7298e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0063 - mae: 0.0552 - val_loss: 0.0050 - val_mae: 0.0458 - learning_rate: 3.7298e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0064 - mae: 0.0551 - val_loss: 0.0055 - val_mae: 0.0498 - learning_rate: 3.7298e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.0549\n",
            "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00018649057892616838.\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - loss: 0.0063 - mae: 0.0549 - val_loss: 0.0053 - val_mae: 0.0510 - learning_rate: 3.7298e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0059 - mae: 0.0531 - val_loss: 0.0045 - val_mae: 0.0433 - learning_rate: 1.8649e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0057 - mae: 0.0526 - val_loss: 0.0048 - val_mae: 0.0468 - learning_rate: 1.8649e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0057 - mae: 0.0522 - val_loss: 0.0043 - val_mae: 0.0430 - learning_rate: 1.8649e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0057 - mae: 0.0525 - val_loss: 0.0045 - val_mae: 0.0438 - learning_rate: 1.8649e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0055 - mae: 0.0518 - val_loss: 0.0047 - val_mae: 0.0453 - learning_rate: 1.8649e-04\n",
            "Epoch 93/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0058 - mae: 0.0533 - val_loss: 0.0041 - val_mae: 0.0415 - learning_rate: 1.8649e-04\n",
            "Epoch 94/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0059 - mae: 0.0533 - val_loss: 0.0041 - val_mae: 0.0414 - learning_rate: 1.8649e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0054 - mae: 0.0511 - val_loss: 0.0056 - val_mae: 0.0513 - learning_rate: 1.8649e-04\n",
            "Epoch 96/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0056 - mae: 0.0521 - val_loss: 0.0043 - val_mae: 0.0436 - learning_rate: 1.8649e-04\n",
            "Epoch 97/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0057 - mae: 0.0539 - val_loss: 0.0040 - val_mae: 0.0412 - learning_rate: 1.8649e-04\n",
            "Epoch 98/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0056 - mae: 0.0523 - val_loss: 0.0043 - val_mae: 0.0440 - learning_rate: 1.8649e-04\n",
            "Epoch 99/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0054 - mae: 0.0515 - val_loss: 0.0043 - val_mae: 0.0436 - learning_rate: 1.8649e-04\n",
            "Epoch 100/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0055 - mae: 0.0522 - val_loss: 0.0045 - val_mae: 0.0457 - learning_rate: 1.8649e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "DNN Test MAE: 2012362.2768, RMSE: 2691417.4755, MSE: 7243728027306.5898, time/sample: 0.000081s\n",
            "Baseline MAE: 2898684.1133, RMSE: 3775600.3207, MSE: 14255157781834.8516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=\"sk-4e98eb1df2dc496dbf9cf38f1aa8320e\",\n",
        "    base_url=\"https://api.deepseek.com\",\n",
        ")\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a resume parser. The user will provide raw resume text.\n",
        "Please extract the following fields and output as a JSON object:\n",
        "- name\n",
        "- email\n",
        "- phone\n",
        "- education (list of strings)\n",
        "- skills (list of strings)\n",
        "\n",
        "Return ONLY valid JSON.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"TAI CONG CHI NGUYEN (JOSEPH)  Saskatoon,  Saskatchewan | 6399983636 | taincc@gmail.c o m | linkedin.com/in/joseph - tainguyen  I  will be graduating  from  Artificial Intelligence and Data Analytics  Post - Graduate Certificate at Saskatchewan Polytechnic and seeking a  Full - time opportunity in Canada. I have been  working  for over 10 years. About 3 years ago, I became interested in the field of  machine  learning,  data engineering  and  business intelligence  then I started focusing my skills on data as well. I am passionate about  understanding user problems and solving them with available resources in the organization.  PROFESSIONAL  WORK  EXPERIENCE  Digital Integration Centre of Excellence (DICE)  ●  Research Assistant  June 2024  -  Present  o  Leverage  machine learning and  geospatial data  to  identify  potential gravel pits  o  Support data collection, cleaning, and preprocessing to prepare for analysis and model development  o  Train  and  test  machine learning models, such as ResNet, VGG16, and Random Forest, using satellite images, hillshade images,  and numerical datasets  o  Contribute to the creation and maintenance  of  PostgreSQL  database  o  Assist in designing visualizations and presenting key findings to partners  TMAM  Project  ●  Part - time Freelancer  January 2024  –  March 2024  o  Develop and maintain a web crawler using Python  o  ETL and  store crawled data  to  MongoDB  and  PostgreSQL  Geniebook  ●  Senior Engineer  April 2022  –  July 2023  o  Design and implement a real - time data pipeline to process structured data from many data sources using Kafka, ksqlDB,  Python and stored processed data in Cassandra  o  Migrate data from MySQL to PostgreSQL for Microservice and Odoo databases  o  Develop Google Forms and use Apps Script to automatically notification from Google Sheets  Amaris Consulting  -  Bolloré Transport & Logistics  -  Mizuho Bank  -  FPT Information System  ●  Experienced Consultant  –  Senior Database Developer  –  Senior Officer  –  Software Developer  September  20 09  –  April 2022  o  Confer with system analysts, developers, and other teams to translate business requirements into data modeling, database  design, efficient SQL for fast application performance  o  Enhancement of existing data migration and consolidation between various databases  o  Troubleshoot database performance related and tune complex database queries  o  Responsible for development and  maintenance of the  core banking system and operating database  o  Participated in development RPA using UIPath  o  Have knowledge of SQL Data Warehouse, ETL by SSIS and Power BI Desktop  o  Expertise in development using  MS SQL Server  and  Oracle PL/SQL  EDUCATION  Saskatchewan Polytechnic  ●  Postgraduate in Artificial Intelligence and Data Analytics  Expected  April 202 5  ●  Postgraduate in Cloud  Computing & Blockchain  April 2024  o  Award:  Saskatchewan Polytechnic Dean ’ s Honour List  University of Greenwich  ●  Bachelor of Science in Computing  20 20  National Institute of IT  ●  Diploma in Software Engineering  2009  CERTIFICATIONS  ●  Machine Learning, Deep Learning + AWS Sagemaker , Udemy  January 2025  ●  RPA  -  Process Automation using UIPATH  -  Beginner to Expert ,  Udemy  April 2022  ●  Master in Microsoft Power BI Desktop and Service ,  Udemy  March 2022  ●  Data Warehouse Developer - SQL Server/ETL/SSIS/SSAS/SSRS/T - SQL , Udemy  March 2022  A DDITIONAL  INFORMATION  ●  Languages:  Vietnamese (native),  English  (conversational proficiency)  ●  Programming Languages:  Python ,  .Net, SQL  ●  Databases:  Oracle, MS SQL, MySQL, PostgreSQL, MongoDB, Cassandra  ●  Stream processing:  Apache Kafka  ●  Cloud Services:  AWS\n",
        "\"\"\"\n",
        "\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=messages,\n",
        "    response_format={\n",
        "        'type': 'json_object'\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.loads(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SuPfjBXkywP",
        "outputId": "68d7f3ec-b560-48c0-e1b4-3f7bfa3f455f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'TAI CONG CHI NGUYEN (JOSEPH)', 'email': 'taincc@gmail.com', 'phone': '6399983636', 'education': ['Postgraduate in Artificial Intelligence and Data Analytics, Saskatchewan Polytechnic, Expected April 2025', 'Postgraduate in Cloud Computing & Blockchain, Saskatchewan Polytechnic, April 2024', 'Bachelor of Science in Computing, University of Greenwich, 2020', 'Diploma in Software Engineering, National Institute of IT, 2009'], 'skills': ['Machine Learning', 'Data Engineering', 'Business Intelligence', 'PostgreSQL', 'Python', 'MongoDB', 'Kafka', 'ksqlDB', 'Cassandra', 'MySQL', 'Google Apps Script', 'SQL', 'Oracle PL/SQL', 'MS SQL Server', 'ETL', 'SSIS', 'Power BI', 'UIPath', 'AWS Sagemaker', 'Apache Kafka', 'AWS', '.Net', 'Vietnamese', 'English']}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdoz3kGSGOrLA8WWlrE3zJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}