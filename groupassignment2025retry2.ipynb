{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanMuInCa/learn_python/blob/master/groupassignment2025retry2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkquzaHtsKA",
        "outputId": "d72b16df-e379-43d7-e78e-0051f8899dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of train data: (64000, 35)\n",
            "Shape after dropping missing values: (64000, 35)\n",
            "All force_meas values are positive.\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 64000 entries, 0 to 63999\n",
            "Data columns (total 35 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   tappingsteelgrade     64000 non-null  object \n",
            " 1   force_meas            64000 non-null  float64\n",
            " 2   speed                 64000 non-null  float64\n",
            " 3   entrytemperature      64000 non-null  float64\n",
            " 4   entrytemperaturebot   64000 non-null  float64\n",
            " 5   entrytemperaturecore  64000 non-null  float64\n",
            " 6   entrytemperaturetop   64000 non-null  float64\n",
            " 7   entrythickness        64000 non-null  float64\n",
            " 8   entrywidth            64000 non-null  float64\n",
            " 9   exitthickness         64000 non-null  float64\n",
            " 10  zeropoint             64000 non-null  float64\n",
            " 11  radius                64000 non-null  float64\n",
            " 12  pctal                 64000 non-null  float64\n",
            " 13  pctb                  64000 non-null  float64\n",
            " 14  pctc                  64000 non-null  float64\n",
            " 15  pctca                 64000 non-null  float64\n",
            " 16  pctco                 64000 non-null  int64  \n",
            " 17  pctcr                 64000 non-null  float64\n",
            " 18  pctcu                 64000 non-null  float64\n",
            " 19  pcth                  64000 non-null  float64\n",
            " 20  pctmg                 64000 non-null  int64  \n",
            " 21  pctmn                 64000 non-null  float64\n",
            " 22  pctmo                 64000 non-null  float64\n",
            " 23  pctn                  64000 non-null  float64\n",
            " 24  pctnb                 64000 non-null  float64\n",
            " 25  pctni                 64000 non-null  float64\n",
            " 26  pcto                  64000 non-null  float64\n",
            " 27  pctp                  64000 non-null  float64\n",
            " 28  pcts                  64000 non-null  float64\n",
            " 29  pctsi                 64000 non-null  float64\n",
            " 30  pctsn                 64000 non-null  float64\n",
            " 31  pctti                 64000 non-null  float64\n",
            " 32  pctv                  64000 non-null  float64\n",
            " 33  pctzr                 64000 non-null  float64\n",
            " 34  fur_line_no           64000 non-null  int64  \n",
            "dtypes: float64(31), int64(3), object(1)\n",
            "memory usage: 17.1+ MB\n",
            "None\n",
            "First 5 rows:\n",
            "  tappingsteelgrade   force_meas     speed  entrytemperature  \\\n",
            "0          GL4G71R1  58204143.19  2.634871       1000.661121   \n",
            "1          GL4G71R1  53211949.94  3.947747       1020.846126   \n",
            "2          JV7P1BP6  43513598.87  1.573130        829.078290   \n",
            "3          JT5P31P2  33293124.30  2.979649        736.655045   \n",
            "4          JT5P32P1  68174217.63  3.401038        893.722030   \n",
            "\n",
            "   entrytemperaturebot  entrytemperaturecore  entrytemperaturetop  \\\n",
            "0           939.942163           1034.327117           916.847574   \n",
            "1           991.621746           1044.907925           975.227384   \n",
            "2           786.696648            857.833946           763.533963   \n",
            "3           734.483511            738.987016           730.511453   \n",
            "4           867.859688            913.164933           850.840106   \n",
            "\n",
            "   entrythickness  entrywidth  exitthickness  ...  pctni    pcto    pctp  \\\n",
            "0        0.080232    3.496171       0.065490  ...  0.004  0.0000  0.0153   \n",
            "1        0.039173    3.974567       0.031835  ...  0.004  0.0000  0.0119   \n",
            "2        0.148507    1.912962       0.136898  ...  0.663  0.0025  0.0088   \n",
            "3        0.014806    2.715973       0.013670  ...  0.019  0.0017  0.0150   \n",
            "4        0.048040    3.654979       0.039451  ...  0.010  0.0019  0.0138   \n",
            "\n",
            "     pcts  pctsi  pctsn   pctti    pctv  pctzr  fur_line_no  \n",
            "0  0.0068  0.135  0.001  0.0097  0.0005  0.000            3  \n",
            "1  0.0051  0.128  0.001  0.0090  0.0005  0.000            4  \n",
            "2  0.0011  0.119  0.001  0.0121  0.0027  0.001            2  \n",
            "3  0.0031  0.283  0.002  0.0128  0.0019  0.001            2  \n",
            "4  0.0040  0.208  0.002  0.0135  0.0011  0.001            3  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (ensure 'train.csv' is in the working directory)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Print the initial shape of the dataset\n",
        "print(\"Initial shape of train data:\", df.shape)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df = df.dropna()\n",
        "print(\"Shape after dropping missing values:\", df.shape)\n",
        "\n",
        "# Check if any force_meas values are non-positive\n",
        "if (df['force_meas'] <= 0).any():\n",
        "    raise ValueError(\"There are non-positive values in force_meas!\")\n",
        "else:\n",
        "    print(\"All force_meas values are positive.\")\n",
        "\n",
        "# Display basic information and the first 5 rows of the dataset\n",
        "print(\"Dataset info:\")\n",
        "print(df.info())\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOpyctu_ulIB",
        "outputId": "e6a16bb1-c48a-4fe8-bafe-e171af65a441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape after encoding: (64000, 56)\n",
            "Feature scaling completed. Sample of scaled features:\n",
            "[[-1.20850089e+00  1.62955293e+00  1.10845858e+00  1.76774467e+00\n",
            "   9.94195846e-01  1.46945502e+00  3.67688850e-01  1.29712105e+00\n",
            "  -2.87615939e-01  1.71820834e+00 -8.44419107e-01 -1.95625464e+00\n",
            "   1.27155016e+00  2.70135712e-01  0.00000000e+00 -5.03154244e-01\n",
            "  -4.02691354e-01  3.55943097e-01  0.00000000e+00 -2.04975855e+00\n",
            "  -8.97415370e-01 -8.22165014e-01 -1.19848532e+00 -3.88496414e-01\n",
            "  -1.45744129e+00  1.43831684e+00  1.67807224e+00 -1.49884912e+00\n",
            "  -7.10231470e-01 -9.64868769e-01 -1.07867137e+00 -1.65212260e+00\n",
            "   4.41799983e-01 -4.35224796e-02  3.13835888e+00 -3.05675752e-01\n",
            "  -6.28734061e-02 -1.60640647e-01 -4.24276982e-02 -1.42536405e-02\n",
            "  -1.26881108e-01 -1.50472361e-01 -3.95399676e-01 -3.48975122e-01\n",
            "  -3.08113970e-01 -2.82561008e-01 -2.24160195e-01 -3.21296526e-02\n",
            "  -1.11937748e-01 -1.76804323e-02 -3.83524497e-02 -1.36943478e-02\n",
            "  -2.72801027e-01 -1.42536405e-02 -3.95399676e-01 -1.81171935e-02]\n",
            " [ 1.78225074e-01  1.91674204e+00  1.89759625e+00  1.90742200e+00\n",
            "   1.91953900e+00 -6.81351499e-02  1.16747200e+00 -1.38752454e-01\n",
            "  -1.21896680e+00  2.62461393e-01 -4.50827118e-01 -1.95625464e+00\n",
            "   9.76890782e-01  8.82576749e-01  0.00000000e+00 -1.30072382e+00\n",
            "  -4.91945734e-01  2.17604202e+00  0.00000000e+00 -2.23618807e+00\n",
            "  -1.20391787e+00 -3.63414240e-01 -1.15923170e+00 -3.88496414e-01\n",
            "  -1.45744129e+00  1.40244327e-01  7.76242495e-01 -1.63754101e+00\n",
            "  -7.10231470e-01 -1.32735470e+00 -1.07867137e+00 -1.65212260e+00\n",
            "   1.33632999e+00 -4.35224796e-02  3.13835888e+00 -3.05675752e-01\n",
            "  -6.28734061e-02 -1.60640647e-01 -4.24276982e-02 -1.42536405e-02\n",
            "  -1.26881108e-01 -1.50472361e-01 -3.95399676e-01 -3.48975122e-01\n",
            "  -3.08113970e-01 -2.82561008e-01 -2.24160195e-01 -3.21296526e-02\n",
            "  -1.11937748e-01 -1.76804323e-02 -3.83524497e-02 -1.36943478e-02\n",
            "  -2.72801027e-01 -1.42536405e-02 -3.95399676e-01 -1.81171935e-02]\n",
            " [-2.32996658e+00 -8.11700890e-01 -1.23157206e+00 -5.62142934e-01\n",
            "  -1.43588574e+00  4.02630544e+00 -2.27912288e+00  4.34373643e+00\n",
            "  -3.08167546e+00 -1.00614129e+00 -1.02715825e+00  1.02932535e+00\n",
            "  -2.17269056e+00  1.32003463e+00  0.00000000e+00  1.86385156e+01\n",
            "   1.40118909e+01 -9.44127566e-01  0.00000000e+00  1.32715799e+00\n",
            "   2.82203220e+01  4.01170384e-01  2.83155345e+00  2.43789853e+01\n",
            "   1.19495667e+00 -1.04329238e+00 -1.34570985e+00 -1.81585915e+00\n",
            "  -7.10231470e-01  2.77940132e-01  1.70703389e+00  5.66014853e-02\n",
            "  -4.52730022e-01 -4.35224796e-02 -3.18637873e-01 -3.05675752e-01\n",
            "  -6.28734061e-02 -1.60640647e-01 -4.24276982e-02 -1.42536405e-02\n",
            "  -1.26881108e-01 -1.50472361e-01 -3.95399676e-01 -3.48975122e-01\n",
            "  -3.08113970e-01 -2.82561008e-01 -2.24160195e-01 -3.21296526e-02\n",
            "  -1.11937748e-01 -1.76804323e-02 -3.83524497e-02 -1.36943478e-02\n",
            "  -2.72801027e-01 -1.42536405e-02 -3.95399676e-01  5.51961869e+01]\n",
            " [-8.44329123e-01 -2.12668444e+00 -2.02885701e+00 -2.13104211e+00\n",
            "  -1.95930563e+00 -9.80660510e-01 -9.36646971e-01 -9.13769905e-01\n",
            "   9.92994514e-01  7.97101306e-01 -2.11860552e-01  1.02932535e+00\n",
            "  -6.43735794e-01 -5.17288480e-01  0.00000000e+00  1.49076970e+00\n",
            "   8.82077323e-02  3.55943097e-01  0.00000000e+00  2.76373423e-01\n",
            "   2.20921254e-02 -2.10497315e-01  6.85688649e-01  1.75255067e-01\n",
            "   3.46189324e-01  1.32378103e+00 -2.84733678e-01  1.43349366e+00\n",
            "   2.82665299e-01  6.40426061e-01  6.94050156e-01  5.66014853e-02\n",
            "  -4.52730022e-01 -4.35224796e-02 -3.18637873e-01 -3.05675752e-01\n",
            "  -6.28734061e-02 -1.60640647e-01 -4.24276982e-02 -1.42536405e-02\n",
            "  -1.26881108e-01 -1.50472361e-01 -3.95399676e-01  2.86553378e+00\n",
            "  -3.08113970e-01 -2.82561008e-01 -2.24160195e-01 -3.21296526e-02\n",
            "  -1.11937748e-01 -1.76804323e-02 -3.83524497e-02 -1.36943478e-02\n",
            "  -2.72801027e-01 -1.42536405e-02 -3.95399676e-01 -1.81171935e-02]\n",
            " [-3.99237269e-01  1.08040179e-01  7.77253749e-03  1.68281812e-01\n",
            "  -5.20487195e-02  2.63898669e-01  6.33183438e-01  1.86171443e-01\n",
            "  -1.71196907e-01 -1.78983328e+00  2.80129434e-01  3.41320212e-02\n",
            "  -1.39348021e+00 -8.67254787e-01  0.00000000e+00  1.61487069e-01\n",
            "  -2.24182595e-01 -6.84113434e-01  0.00000000e+00  4.75514047e-01\n",
            "   3.28594624e-01 -1.89258349e+00  4.37082361e-01 -1.62995822e-01\n",
            "   5.58381161e-01  8.65637792e-01  1.92705600e-01 -5.24908552e-02\n",
            "   2.82665299e-01  1.00291199e+00 -3.18933575e-01  5.66014853e-02\n",
            "   4.41799983e-01 -4.35224796e-02 -3.18637873e-01 -3.05675752e-01\n",
            "  -6.28734061e-02 -1.60640647e-01 -4.24276982e-02 -1.42536405e-02\n",
            "  -1.26881108e-01 -1.50472361e-01 -3.95399676e-01 -3.48975122e-01\n",
            "  -3.08113970e-01  3.53905872e+00 -2.24160195e-01 -3.21296526e-02\n",
            "  -1.11937748e-01 -1.76804323e-02 -3.83524497e-02 -1.36943478e-02\n",
            "  -2.72801027e-01 -1.42536405e-02 -3.95399676e-01 -1.81171935e-02]]\n"
          ]
        }
      ],
      "source": [
        "# Select features by dropping the target column 'force_meas'\n",
        "features = df.drop(columns=['force_meas'])\n",
        "target = df['force_meas']\n",
        "\n",
        "# Convert the categorical column 'tappingsteelgrade' using one-hot encoding\n",
        "if 'tappingsteelgrade' in features.columns:\n",
        "    features = pd.get_dummies(features, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "print(\"Features shape after encoding:\", features.shape)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "print(\"Feature scaling completed. Sample of scaled features:\")\n",
        "print(X_scaled[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMrZCqUFvIeQ",
        "outputId": "2d14806d-d909-40cc-b480-a96d3280a96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression CV MAE: 5531112.7093\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Split the scaled features and target into training and validation sets\n",
        "# 注意：features_scaled 是经过之前预处理与特征缩放后的数据（形状为 (18740, 56)），\n",
        "# y 是目标变量 force_meas\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, df['force_meas'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr_cv_scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
        "print(\"Linear Regression CV MAE: {:.4f}\".format(-np.mean(lr_cv_scores)))\n",
        "\n",
        "# -------------------------------\n",
        "# Decision Tree Regressor with Grid Search\n",
        "# -------------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "dt_grid = GridSearchCV(dt, dt_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Decision Tree CV MAE: {:.4f}\".format(-dt_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Random Forest Regressor with Grid Search\n",
        "# -------------------------------\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Random Forest CV MAE: {:.4f}\".format(-rf_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient Boosting Regressor with Grid Search\n",
        "# -------------------------------\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "gb_grid = GridSearchCV(gb, gb_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "print(\"Gradient Boosting best params:\", gb_grid.best_params_)\n",
        "print(\"Gradient Boosting CV MAE: {:.4f}\".format(-gb_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate each model on the validation set\n",
        "# -------------------------------\n",
        "models = {\n",
        "    'Linear Regression': lr,\n",
        "    'Decision Tree': dt_grid.best_estimator_,\n",
        "    'Random Forest': rf_grid.best_estimator_,\n",
        "    'Gradient Boosting': gb_grid.best_estimator_\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    print(f\"{name} Validation MAE: {mae:.4f}\")\n",
        "    print(f\"{name} Validation RMSE: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-y3N79LFJqH"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the fitted scaler for later use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)  # Save the scaler object\n",
        "\n",
        "# Save the best Random Forest model (obtained from GridSearchCV)\n",
        "with open('best_rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_grid.best_estimator_, f)  # Save the best RF model\n",
        "\n",
        "# Save the best Gradient Boosting model (obtained from GridSearchCV)\n",
        "with open('best_gb_model.pkl', 'wb') as f:\n",
        "    pickle.dump(gb_grid.best_estimator_, f)  # Save the best GB model\n",
        "\n",
        "# Save the training columns (features names) for later use\n",
        "training_columns = features.columns.tolist()  # 'features' 是经过预处理后用于训练的 DataFrame\n",
        "with open('training_columns.pkl', 'wb') as f:\n",
        "    pickle.dump(training_columns, f)\n",
        "print(\"Training columns have been saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HIzk4uFPRDc"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# Step 11: Final Evaluation on Test Data\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load test data (ensure test.csv is in your working directory)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Drop missing values if any (you may adjust based on your preprocessing strategy)\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# For test data, note: do not use the 'force_pre' column for prediction, it's the baseline.\n",
        "# We assume test_df contains 'force_pre' and other features including 'tappingsteelgrade'\n",
        "# Also assume that 'force_meas' is present for evaluation\n",
        "# If not, adjust accordingly (for final evaluation, force_meas is needed)\n",
        "\n",
        "# Separate baseline predictions for later comparison\n",
        "baseline_force_pre = test_df['force_pre']\n",
        "\n",
        "# Separate target variable for evaluation (force_meas)\n",
        "y_test = test_df['force_meas']\n",
        "\n",
        "# ===============================\n",
        "# Preprocessing: Feature Selection & Encoding\n",
        "# ===============================\n",
        "# Exclude the target 'force_meas' and baseline column 'force_pre'\n",
        "features_test = test_df.drop(columns=['force_meas', 'force_pre'])\n",
        "\n",
        "# One-hot encode the categorical column 'tappingsteelgrade' (using the same columns as during training)\n",
        "features_test = pd.get_dummies(features_test, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "# IMPORTANT: Align the test features with the training features.\n",
        "# Load the fitted scaler (assumes scaler.pkl was saved during training)\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# To ensure that test data has the same feature columns as training data,\n",
        "# we reindex the test DataFrame to match training columns.\n",
        "# Assume that 'training_columns' was saved during training.\n",
        "with open('training_columns.pkl', 'rb') as f:\n",
        "    training_columns = pickle.load(f)\n",
        "\n",
        "# Reindex the test features DataFrame; missing columns will be filled with zeros.\n",
        "features_test = features_test.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "# Scale the test features using the saved scaler\n",
        "X_test_scaled = scaler.transform(features_test)\n",
        "\n",
        "# ===============================\n",
        "# Load the saved models\n",
        "# ===============================\n",
        "# Load best Random Forest model\n",
        "with open('best_rf_model.pkl', 'rb') as f:\n",
        "    best_rf_model = pickle.load(f)\n",
        "\n",
        "# Load best Gradient Boosting model\n",
        "with open('best_gb_model.pkl', 'rb') as f:\n",
        "    best_gb_model = pickle.load(f)\n",
        "\n",
        "# ===============================\n",
        "# Evaluate Models on Test Data\n",
        "# ===============================\n",
        "# Evaluate Random Forest model\n",
        "start_time = time.time()\n",
        "rf_predictions = best_rf_model.predict(X_test_scaled)\n",
        "rf_runtime = (time.time() - start_time) / len(X_test_scaled)  # Average prediction time per sample\n",
        "\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Evaluate Gradient Boosting model\n",
        "start_time = time.time()\n",
        "gb_predictions = best_gb_model.predict(X_test_scaled)\n",
        "gb_runtime = (time.time() - start_time) / len(X_test_scaled)\n",
        "\n",
        "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
        "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
        "gb_mse = mean_squared_error(y_test, gb_predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"Random Forest - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    rf_mae, rf_rmse, rf_mse, rf_runtime))\n",
        "print(\"Gradient Boosting - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    gb_mae, gb_rmse, gb_mse, gb_runtime))\n",
        "\n",
        "# Compare with baseline force_pre (if available)\n",
        "# Calculate baseline evaluation metrics using force_pre column\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_force_pre)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_force_pre))\n",
        "baseline_mse = mean_squared_error(y_test, baseline_force_pre)\n",
        "\n",
        "print(\"Baseline (force_pre) - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}\".format(\n",
        "    baseline_mae, baseline_rmse, baseline_mse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEnNFrcQUqrb"
      },
      "outputs": [],
      "source": [
        "# Install Keras Tuner if not already installed\n",
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "\n",
        "# Define a model-building function for the tuner\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Build a DNN model with hyperparameters to tune.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Input layer using an Input layer is preferred\n",
        "    model.add(keras.Input(shape=(X_train.shape[1],)))  # X_train为训练集特征，确保在运行前定义好\n",
        "\n",
        "    # Tune the number of layers (at least 1 layer)\n",
        "    num_layers = hp.Int('num_layers', 1, 3, default=2)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        # Tune the number of units in this layer\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32, default=64)\n",
        "        # Tune dropout rate for this layer\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1, default=0.2)\n",
        "        # Tune L2 regularization factor\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', 1e-5, 1e-3, sampling='LOG', default=1e-4)\n",
        "\n",
        "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate for the optimizer\n",
        "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Set up the tuner: Here we use Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='dnn_tuner_dir',\n",
        "    project_name='force_prediction_tuning'\n",
        ")\n",
        "\n",
        "# Optional: Early stopping callback to stop training if no improvement\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best hyperparameters and build the best model\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model and train it\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "history = best_model.fit(X_train, y_train,\n",
        "                         epochs=50,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[stop_early],\n",
        "                         verbose=1)\n",
        "\n",
        "# 保存最佳模型及相关Scaler等（确保之前训练时保存了Scaler和训练列）\n",
        "best_model.save('optimized_dnn_model.h5')\n",
        "print(\"Optimized DNN model has been saved successfully.\")\n",
        "\n",
        "# 若需要后续加载最佳模型，请使用以下代码：\n",
        "# loaded_best_model = tf.keras.models.load_model('optimized_dnn_model.h5', compile=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgxTtM1kf9A2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Assume that X_train, y_train, X_val, and y_val are already preprocessed\n",
        "# and that y_train and y_val have been log-transformed\n",
        "y_train_log = np.log(y_train)\n",
        "y_val_log = np.log(y_val)\n",
        "\n",
        "# Get the number of features from X_train\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Define the model-building function for the tuner\n",
        "def build_advanced_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "    # Loop to add multiple hidden layers (between 4 and 6 layers)\n",
        "    for i in range(hp.Int('num_layers', 4, 6)):\n",
        "        # Define the number of units in the layer (range: 32 to 512, step size: 32)\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=512, step=32)\n",
        "        # Choose the activation function: ReLU or LeakyReLU\n",
        "        activation_choice = hp.Choice(f'activation_{i}', values=['relu', 'leaky_relu'])\n",
        "        # Define dropout rate (from 0.0 to 0.5)\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Define L2 regularization factor (using log sampling)\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "        # Add a Dense layer with L2 regularization\n",
        "        model.add(layers.Dense(units, kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "\n",
        "        # Add the chosen activation function\n",
        "        if activation_choice == 'leaky_relu':\n",
        "            model.add(layers.LeakyReLU(alpha=0.1))\n",
        "        else:\n",
        "            model.add(layers.Activation('relu'))\n",
        "\n",
        "        # Add a Dropout layer\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "        # Optionally add Batch Normalization after each layer\n",
        "        if hp.Boolean(f'batchnorm_{i}'):\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Output layer for predicting log(force_meas)\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Define the learning rate\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model with MSE loss and MAE metric\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Use Keras Tuner for hyperparameter tuning with RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_advanced_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=30,\n",
        "    executions_per_trial=1,\n",
        "    directory='advanced_dnn_tuning',\n",
        "    project_name='rolling_force_advanced'\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search(X_train, y_train_log, epochs=50, validation_data=(X_val, y_val_log))\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model using the best hyperparameters\n",
        "best_advanced_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "# Define callbacks for learning rate reduction and early stopping\n",
        "lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the best model\n",
        "history = best_advanced_model.fit(X_train, y_train_log,\n",
        "                                  epochs=100,\n",
        "                                  validation_data=(X_val, y_val_log),\n",
        "                                  callbacks=[lr_reducer, early_stopper])\n",
        "\n",
        "# Save the optimized model (using HDF5 format; you can also use the native Keras format)\n",
        "best_advanced_model.save(\"optimized_dnn_model_advanced_log.h5\")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate the optimized model on the validation set\n",
        "# -------------------------------\n",
        "# Predict log-transformed values on validation set and revert the transformation\n",
        "y_val_pred_log = best_advanced_model.predict(X_val).flatten()\n",
        "y_val_pred = np.exp(y_val_pred_log)  # Reverse log transformation\n",
        "y_val_orig = np.exp(y_val_log)\n",
        "\n",
        "# Calculate evaluation metrics on the original scale\n",
        "val_mae = mean_absolute_error(y_val_orig, y_val_pred)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_orig, y_val_pred))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation MAE: {:.4f}\".format(val_mae))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation RMSE: {:.4f}\".format(val_rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 1: Load saved preprocessing artifacts\n",
        "# ---------------------------------------\n",
        "# Load the scaler used for the DNN (after applying log transformation on the target)\n",
        "with open('scaler_dnn_log.pkl', 'rb') as f:\n",
        "    scaler_dnn_log = pickle.load(f)\n",
        "\n",
        "# Load the training columns saved during training (to ensure correct feature ordering)\n",
        "with open('training_columns.pkl', 'rb') as f:\n",
        "    training_columns = pickle.load(f)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 2: Load the Advanced Optimized DNN Model\n",
        "# ---------------------------------------\n",
        "# Note: Provide any required custom_objects if using custom activations, e.g., 'leaky_relu'\n",
        "custom_objects = {'leaky_relu': tf.keras.layers.LeakyReLU}\n",
        "advanced_dnn = tf.keras.models.load_model('advanced_optimized_dnn_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "print(\"Advanced optimized DNN model loaded successfully.\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 3: Preprocess the Test Data\n",
        "# ---------------------------------------\n",
        "# Assume test_df is a DataFrame containing the raw test features (after initial cleaning and encoding)\n",
        "# Ensure the test DataFrame contains all the features used in training and in the same order\n",
        "features_test = test_df[training_columns]  # training_columns ensures correct feature order\n",
        "\n",
        "# Scale the test features using the loaded scaler\n",
        "X_test_dnn = scaler_dnn_log.transform(features_test)\n",
        "\n",
        "# Assume y_test contains the true force_meas values for the test set\n",
        "# (注意：如果在训练时对 force_meas 进行了对数变换，预测时需要进行逆变换)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Step 4: Evaluate the Model on Test Data\n",
        "# ---------------------------------------\n",
        "start_time = time.time()\n",
        "# Obtain predictions in log-scale\n",
        "dnn_predictions_log = advanced_dnn.predict(X_test_dnn)\n",
        "# Reverse the log transformation (assume natural logarithm was applied during training)\n",
        "dnn_predictions = np.exp(dnn_predictions_log)\n",
        "dnn_runtime = (time.time() - start_time) / X_test_dnn.shape[0]  # average runtime per sample\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mae_dnn = mean_absolute_error(y_test, dnn_predictions)\n",
        "rmse_dnn = np.sqrt(mean_squared_error(y_test, dnn_predictions))\n",
        "\n",
        "print(\"Advanced Optimized DNN (with log transformation) Test MAE: {:.4f}\".format(mae_dnn))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Test RMSE: {:.4f}\".format(rmse_dnn))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Average prediction runtime per sample: {:.6f} seconds\".format(dnn_runtime))\n"
      ],
      "metadata": {
        "id": "s5Dk6jysa20U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMudJPW934ixWCye3gNc2w2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}