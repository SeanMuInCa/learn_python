{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnXzQY5xUGgVMfcIKWbFSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanMuInCa/learn_python/blob/master/groupassignment2025retry2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkquzaHtsKA",
        "outputId": "63a3c209-49a6-4bab-f2aa-61d9ae8d24ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of train data: (14994, 35)\n",
            "Shape after dropping missing values: (14993, 35)\n",
            "All force_meas values are positive.\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 14993 entries, 0 to 14992\n",
            "Data columns (total 35 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   tappingsteelgrade     14993 non-null  object \n",
            " 1   force_meas            14993 non-null  float64\n",
            " 2   speed                 14993 non-null  float64\n",
            " 3   entrytemperature      14993 non-null  float64\n",
            " 4   entrytemperaturebot   14993 non-null  float64\n",
            " 5   entrytemperaturecore  14993 non-null  float64\n",
            " 6   entrytemperaturetop   14993 non-null  float64\n",
            " 7   entrythickness        14993 non-null  float64\n",
            " 8   entrywidth            14993 non-null  float64\n",
            " 9   exitthickness         14993 non-null  float64\n",
            " 10  zeropoint             14993 non-null  float64\n",
            " 11  radius                14993 non-null  float64\n",
            " 12  pctal                 14993 non-null  float64\n",
            " 13  pctb                  14993 non-null  float64\n",
            " 14  pctc                  14993 non-null  float64\n",
            " 15  pctca                 14993 non-null  float64\n",
            " 16  pctco                 14993 non-null  float64\n",
            " 17  pctcr                 14993 non-null  float64\n",
            " 18  pctcu                 14993 non-null  float64\n",
            " 19  pcth                  14993 non-null  float64\n",
            " 20  pctmg                 14993 non-null  float64\n",
            " 21  pctmn                 14993 non-null  float64\n",
            " 22  pctmo                 14993 non-null  float64\n",
            " 23  pctn                  14993 non-null  float64\n",
            " 24  pctnb                 14993 non-null  float64\n",
            " 25  pctni                 14993 non-null  float64\n",
            " 26  pcto                  14993 non-null  float64\n",
            " 27  pctp                  14993 non-null  float64\n",
            " 28  pcts                  14993 non-null  float64\n",
            " 29  pctsi                 14993 non-null  float64\n",
            " 30  pctsn                 14993 non-null  float64\n",
            " 31  pctti                 14993 non-null  float64\n",
            " 32  pctv                  14993 non-null  float64\n",
            " 33  pctzr                 14993 non-null  float64\n",
            " 34  fur_line_no           14993 non-null  float64\n",
            "dtypes: float64(34), object(1)\n",
            "memory usage: 4.1+ MB\n",
            "None\n",
            "First 5 rows:\n",
            "  tappingsteelgrade   force_meas     speed  entrytemperature  \\\n",
            "0          GL4G71R1  58204143.19  2.634871       1000.661121   \n",
            "1          GL4G71R1  53211949.94  3.947747       1020.846126   \n",
            "2          JV7P1BP6  43513598.87  1.573130        829.078290   \n",
            "3          JT5P31P2  33293124.30  2.979649        736.655045   \n",
            "4          JT5P32P1  68174217.63  3.401038        893.722030   \n",
            "\n",
            "   entrytemperaturebot  entrytemperaturecore  entrytemperaturetop  \\\n",
            "0           939.942163           1034.327117           916.847574   \n",
            "1           991.621746           1044.907925           975.227384   \n",
            "2           786.696648            857.833946           763.533963   \n",
            "3           734.483511            738.987016           730.511453   \n",
            "4           867.859688            913.164933           850.840106   \n",
            "\n",
            "   entrythickness  entrywidth  exitthickness  ...  pctni    pcto    pctp  \\\n",
            "0        0.080232    3.496171       0.065490  ...  0.004  0.0000  0.0153   \n",
            "1        0.039173    3.974567       0.031835  ...  0.004  0.0000  0.0119   \n",
            "2        0.148507    1.912962       0.136898  ...  0.663  0.0025  0.0088   \n",
            "3        0.014806    2.715973       0.013670  ...  0.019  0.0017  0.0150   \n",
            "4        0.048040    3.654979       0.039451  ...  0.010  0.0019  0.0138   \n",
            "\n",
            "     pcts  pctsi  pctsn   pctti    pctv  pctzr  fur_line_no  \n",
            "0  0.0068  0.135  0.001  0.0097  0.0005  0.000          3.0  \n",
            "1  0.0051  0.128  0.001  0.0090  0.0005  0.000          4.0  \n",
            "2  0.0011  0.119  0.001  0.0121  0.0027  0.001          2.0  \n",
            "3  0.0031  0.283  0.002  0.0128  0.0019  0.001          2.0  \n",
            "4  0.0040  0.208  0.002  0.0135  0.0011  0.001          3.0  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (ensure 'train.csv' is in the working directory)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Print the initial shape of the dataset\n",
        "print(\"Initial shape of train data:\", df.shape)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df = df.dropna()\n",
        "print(\"Shape after dropping missing values:\", df.shape)\n",
        "\n",
        "# Check if any force_meas values are non-positive\n",
        "if (df['force_meas'] <= 0).any():\n",
        "    raise ValueError(\"There are non-positive values in force_meas!\")\n",
        "else:\n",
        "    print(\"All force_meas values are positive.\")\n",
        "\n",
        "# Display basic information and the first 5 rows of the dataset\n",
        "print(\"Dataset info:\")\n",
        "print(df.info())\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features by dropping the target column 'force_meas'\n",
        "features = df.drop(columns=['force_meas'])\n",
        "target = df['force_meas']\n",
        "\n",
        "# Convert the categorical column 'tappingsteelgrade' using one-hot encoding\n",
        "if 'tappingsteelgrade' in features.columns:\n",
        "    features = pd.get_dummies(features, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "print(\"Features shape after encoding:\", features.shape)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "print(\"Feature scaling completed. Sample of scaled features:\")\n",
        "print(X_scaled[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOpyctu_ulIB",
        "outputId": "4a77803f-075f-4e77-ee41-11eccf6471c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape after encoding: (14993, 56)\n",
            "Feature scaling completed. Sample of scaled features:\n",
            "[[-1.20815045e+00  1.62317851e+00  1.10172575e+00  1.76238659e+00\n",
            "   9.87948550e-01  1.45623695e+00  3.61472968e-01  1.28464984e+00\n",
            "  -2.83133583e-01  1.73464961e+00 -8.51661554e-01 -1.95072152e+00\n",
            "   1.28237237e+00  2.64962246e-01  0.00000000e+00 -4.82318706e-01\n",
            "  -3.95594767e-01  3.58462510e-01  0.00000000e+00 -2.04614134e+00\n",
            "  -8.06154473e-01 -8.24254455e-01 -1.19837786e+00 -3.61909645e-01\n",
            "  -1.46499377e+00  1.43656069e+00  1.66689327e+00 -1.48847140e+00\n",
            "  -7.13871890e-01 -9.52200096e-01 -1.07879267e+00 -1.65922425e+00\n",
            "   4.48053053e-01 -4.08684453e-02  3.14958465e+00 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [ 1.81403894e-01  1.90906713e+00  1.88553755e+00  1.90154481e+00\n",
            "   1.90664373e+00 -6.79742133e-02  1.16145056e+00 -1.37449236e-01\n",
            "  -1.21715550e+00  2.67468694e-01 -4.56714531e-01 -1.95072152e+00\n",
            "   9.87407966e-01  8.76962173e-01  0.00000000e+00 -1.23416723e+00\n",
            "  -4.81487521e-01  2.18056736e+00  0.00000000e+00 -2.23225175e+00\n",
            "  -1.07843205e+00 -3.69278862e-01 -1.15916458e+00 -3.61909645e-01\n",
            "  -1.46499377e+00  1.40539080e-01  7.72697106e-01 -1.62638831e+00\n",
            "  -7.13871890e-01 -1.31325026e+00 -1.07879267e+00 -1.65922425e+00\n",
            "   1.34697710e+00 -4.08684453e-02  3.14958465e+00 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [-2.33190350e+00 -8.07020499e-01 -1.22251207e+00 -5.58842097e-01\n",
            "  -1.42467451e+00  3.99083949e+00 -2.28598226e+00  4.30203882e+00\n",
            "  -3.08520629e+00 -1.01109806e+00 -1.03502982e+00  1.01953925e+00\n",
            "  -2.16543376e+00  1.31410498e+00  0.00000000e+00  1.75620458e+01\n",
            "   1.34760850e+01 -9.43040956e-01  0.00000000e+00  1.32499496e+00\n",
            "   2.50602156e+01  3.89013792e-01  2.82751950e+00  2.20562425e+01\n",
            "   1.19251775e+00 -1.04112768e+00 -1.33129387e+00 -1.80371007e+00\n",
            "  -7.13871890e-01  2.85686174e-01  1.73109265e+00  6.70091258e-02\n",
            "  -4.50870997e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01  3.87078803e+01]\n",
            " [-8.43235919e-01 -2.11604937e+00 -2.01441616e+00 -2.12191053e+00\n",
            "  -1.94433398e+00 -9.72559432e-01 -9.43179965e-01 -9.05031891e-01\n",
            "   1.00114958e+00  8.06307865e-01 -2.16925267e-01  1.01953925e+00\n",
            "  -6.34896246e-01 -5.21894803e-01  0.00000000e+00  1.39730259e+00\n",
            "   7.68153799e-02  3.58462510e-01  0.00000000e+00  2.76009007e-01\n",
            "   1.06782643e-02 -2.17620332e-01  6.83859865e-01  1.48366958e-01\n",
            "   3.42114062e-01  1.32220584e+00 -2.79298382e-01  1.42748629e+00\n",
            "   2.73952867e-01  6.46736336e-01  7.09316169e-01  6.70091258e-02\n",
            "  -4.50870997e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01  2.89827535e+00\n",
            "  -3.10709877e-01 -2.82072849e-01 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]\n",
            " [-3.97236250e-01  1.08555675e-01  8.46821474e-03  1.68868059e-01\n",
            "  -5.07794442e-02  2.61170485e-01  6.27032103e-01  1.84357643e-01\n",
            "  -1.66380670e-01 -1.80094545e+00  2.76758512e-01  2.94523289e-02\n",
            "  -1.38541678e+00 -8.71609047e-01  0.00000000e+00  1.44221726e-01\n",
            "  -2.23809259e-01 -6.82740263e-01  0.00000000e+00  4.74808764e-01\n",
            "   2.82955844e-01 -1.88586417e+00  4.35509054e-01 -1.57799004e-01\n",
            "   5.54714983e-01  8.64786451e-01  1.94099588e-01 -5.01949719e-02\n",
            "   2.73952867e-01  1.00778650e+00 -3.12460310e-01  6.70091258e-02\n",
            "   4.48053053e-01 -4.08684453e-02 -3.17502182e-01 -3.00138110e-01\n",
            "  -6.74989919e-02 -1.63422278e-01 -3.16459922e-02 -1.63359226e-02\n",
            "  -1.25915578e-01 -1.50482932e-01 -3.93355942e-01 -3.45032780e-01\n",
            "  -3.10709877e-01  3.54518346e+00 -2.28342617e-01 -3.46699198e-02\n",
            "  -1.09612270e-01 -1.63359226e-02 -3.65477450e-02 -1.15504710e-02\n",
            "  -2.73717255e-01 -1.82647259e-02 -3.97865908e-01 -2.58345327e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Split the scaled features and target into training and validation sets\n",
        "# 注意：features_scaled 是经过之前预处理与特征缩放后的数据（形状为 (18740, 56)），\n",
        "# y 是目标变量 force_meas\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, df['force_meas'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr_cv_scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
        "print(\"Linear Regression CV MAE: {:.4f}\".format(-np.mean(lr_cv_scores)))\n",
        "\n",
        "# -------------------------------\n",
        "# Decision Tree Regressor with Grid Search\n",
        "# -------------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "dt_grid = GridSearchCV(dt, dt_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Decision Tree CV MAE: {:.4f}\".format(-dt_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Random Forest Regressor with Grid Search\n",
        "# -------------------------------\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Random Forest CV MAE: {:.4f}\".format(-rf_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient Boosting Regressor with Grid Search\n",
        "# -------------------------------\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "gb_grid = GridSearchCV(gb, gb_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "print(\"Gradient Boosting best params:\", gb_grid.best_params_)\n",
        "print(\"Gradient Boosting CV MAE: {:.4f}\".format(-gb_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate each model on the validation set\n",
        "# -------------------------------\n",
        "models = {\n",
        "    'Linear Regression': lr,\n",
        "    'Decision Tree': dt_grid.best_estimator_,\n",
        "    'Random Forest': rf_grid.best_estimator_,\n",
        "    'Gradient Boosting': gb_grid.best_estimator_\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    print(f\"{name} Validation MAE: {mae:.4f}\")\n",
        "    print(f\"{name} Validation RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMrZCqUFvIeQ",
        "outputId": "a920b2d0-9ca0-46c7-e1e1-8b7d973306c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression CV MAE: 5565531.4078\n",
            "Decision Tree best params: {'max_depth': 10, 'min_samples_split': 10}\n",
            "Decision Tree CV MAE: 6599853.9875\n",
            "Random Forest best params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Random Forest CV MAE: 5252093.7735\n",
            "Gradient Boosting best params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "Gradient Boosting CV MAE: 4989130.7414\n",
            "Linear Regression Validation MAE: 5567526.7685\n",
            "Linear Regression Validation RMSE: 7214382.6856\n",
            "Decision Tree Validation MAE: 6520268.7094\n",
            "Decision Tree Validation RMSE: 8655189.8080\n",
            "Random Forest Validation MAE: 5127791.6150\n",
            "Random Forest Validation RMSE: 6684445.4882\n",
            "Gradient Boosting Validation MAE: 5006398.8995\n",
            "Gradient Boosting Validation RMSE: 6425564.8506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fitted scaler for later use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)  # Save the scaler object\n",
        "\n",
        "# Save the best Random Forest model (obtained from GridSearchCV)\n",
        "with open('best_rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_grid.best_estimator_, f)  # Save the best RF model\n",
        "\n",
        "# Save the best Gradient Boosting model (obtained from GridSearchCV)\n",
        "with open('best_gb_model.pkl', 'wb') as f:\n",
        "    pickle.dump(gb_grid.best_estimator_, f)  # Save the best GB model\n",
        "\n",
        "# Save the training columns (features names) for later use\n",
        "training_columns = features.columns.tolist()  # 'features' 是经过预处理后用于训练的 DataFrame\n",
        "with open('training_columns.pkl', 'wb') as f:\n",
        "    pickle.dump(training_columns, f)\n",
        "print(\"Training columns have been saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-dYyPnxRSKe",
        "outputId": "86faf63b-9a5c-4295-8226-fcff3e215987"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training columns have been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Step 11: Final Evaluation on Test Data\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load test data (ensure test.csv is in your working directory)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Drop missing values if any (you may adjust based on your preprocessing strategy)\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# For test data, note: do not use the 'force_pre' column for prediction, it's the baseline.\n",
        "# We assume test_df contains 'force_pre' and other features including 'tappingsteelgrade'\n",
        "# Also assume that 'force_meas' is present for evaluation\n",
        "# If not, adjust accordingly (for final evaluation, force_meas is needed)\n",
        "\n",
        "# Separate baseline predictions for later comparison\n",
        "baseline_force_pre = test_df['force_pre']\n",
        "\n",
        "# Separate target variable for evaluation (force_meas)\n",
        "y_test = test_df['force_meas']\n",
        "\n",
        "# ===============================\n",
        "# Preprocessing: Feature Selection & Encoding\n",
        "# ===============================\n",
        "# Exclude the target 'force_meas' and baseline column 'force_pre'\n",
        "features_test = test_df.drop(columns=['force_meas', 'force_pre'])\n",
        "\n",
        "# One-hot encode the categorical column 'tappingsteelgrade' (using the same columns as during training)\n",
        "features_test = pd.get_dummies(features_test, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "# IMPORTANT: Align the test features with the training features.\n",
        "# Load the fitted scaler (assumes scaler.pkl was saved during training)\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# To ensure that test data has the same feature columns as training data,\n",
        "# we reindex the test DataFrame to match training columns.\n",
        "# Assume that 'training_columns' was saved during training.\n",
        "with open('training_columns.pkl', 'rb') as f:\n",
        "    training_columns = pickle.load(f)\n",
        "\n",
        "# Reindex the test features DataFrame; missing columns will be filled with zeros.\n",
        "features_test = features_test.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "# Scale the test features using the saved scaler\n",
        "X_test_scaled = scaler.transform(features_test)\n",
        "\n",
        "# ===============================\n",
        "# Load the saved models\n",
        "# ===============================\n",
        "# Load best Random Forest model\n",
        "with open('best_rf_model.pkl', 'rb') as f:\n",
        "    best_rf_model = pickle.load(f)\n",
        "\n",
        "# Load best Gradient Boosting model\n",
        "with open('best_gb_model.pkl', 'rb') as f:\n",
        "    best_gb_model = pickle.load(f)\n",
        "\n",
        "# ===============================\n",
        "# Evaluate Models on Test Data\n",
        "# ===============================\n",
        "# Evaluate Random Forest model\n",
        "start_time = time.time()\n",
        "rf_predictions = best_rf_model.predict(X_test_scaled)\n",
        "rf_runtime = (time.time() - start_time) / len(X_test_scaled)  # Average prediction time per sample\n",
        "\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Evaluate Gradient Boosting model\n",
        "start_time = time.time()\n",
        "gb_predictions = best_gb_model.predict(X_test_scaled)\n",
        "gb_runtime = (time.time() - start_time) / len(X_test_scaled)\n",
        "\n",
        "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
        "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
        "gb_mse = mean_squared_error(y_test, gb_predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"Random Forest - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    rf_mae, rf_rmse, rf_mse, rf_runtime))\n",
        "print(\"Gradient Boosting - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    gb_mae, gb_rmse, gb_mse, gb_runtime))\n",
        "\n",
        "# Compare with baseline force_pre (if available)\n",
        "# Calculate baseline evaluation metrics using force_pre column\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_force_pre)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_force_pre))\n",
        "baseline_mse = mean_squared_error(y_test, baseline_force_pre)\n",
        "\n",
        "print(\"Baseline (force_pre) - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}\".format(\n",
        "    baseline_mae, baseline_rmse, baseline_mse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HIzk4uFPRDc",
        "outputId": "c8f2e2aa-22ad-4436-baf3-f936e1b68b1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Metrics:\n",
            "Random Forest - MAE: 5055189.8127, RMSE: 6575285.0083, MSE: 43234372940812.3203, Avg Runtime per sample: 0.000022 sec\n",
            "Gradient Boosting - MAE: 4952478.7807, RMSE: 6351928.0018, MSE: 40346989340435.9062, Avg Runtime per sample: 0.000003 sec\n",
            "Baseline (force_pre) - MAE: 2898684.1133, RMSE: 3775600.3207, MSE: 14255157781834.8516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner if not already installed\n",
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "\n",
        "# Define a model-building function for the tuner\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Build a DNN model with hyperparameters to tune.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Input layer using an Input layer is preferred\n",
        "    model.add(keras.Input(shape=(X_train.shape[1],)))  # X_train为训练集特征，确保在运行前定义好\n",
        "\n",
        "    # Tune the number of layers (at least 1 layer)\n",
        "    num_layers = hp.Int('num_layers', 1, 3, default=2)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        # Tune the number of units in this layer\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32, default=64)\n",
        "        # Tune dropout rate for this layer\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1, default=0.2)\n",
        "        # Tune L2 regularization factor\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', 1e-5, 1e-3, sampling='LOG', default=1e-4)\n",
        "\n",
        "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate for the optimizer\n",
        "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Set up the tuner: Here we use Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='dnn_tuner_dir',\n",
        "    project_name='force_prediction_tuning'\n",
        ")\n",
        "\n",
        "# Optional: Early stopping callback to stop training if no improvement\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best hyperparameters and build the best model\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model and train it\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "history = best_model.fit(X_train, y_train,\n",
        "                         epochs=50,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[stop_early],\n",
        "                         verbose=1)\n",
        "\n",
        "# 保存最佳模型及相关Scaler等（确保之前训练时保存了Scaler和训练列）\n",
        "best_model.save('optimized_dnn_model.h5')\n",
        "print(\"Optimized DNN model has been saved successfully.\")\n",
        "\n",
        "# 若需要后续加载最佳模型，请使用以下代码：\n",
        "# loaded_best_model = tf.keras.models.load_model('optimized_dnn_model.h5', compile=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEnNFrcQUqrb",
        "outputId": "751663cb-48db-4d77-9a7a-56e0ef68838a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 89 Complete [00h 01m 10s]\n",
            "val_mae: 50453676.0\n",
            "\n",
            "Best val_mae So Far: 47161284.0\n",
            "Total elapsed time: 00h 29m 07s\n",
            "\n",
            "Search: Running Trial #90\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "2                 |3                 |num_layers\n",
            "192               |64                |units_0\n",
            "0.3               |0.3               |dropout_0\n",
            "0.00023201        |1.1315e-05        |l2_reg_0\n",
            "256               |224               |units_1\n",
            "0.4               |0.2               |dropout_1\n",
            "0.00051684        |4.1139e-05        |l2_reg_1\n",
            "0.00065826        |0.0067641         |learning_rate\n",
            "256               |256               |units_2\n",
            "0.1               |0.2               |dropout_2\n",
            "2.4462e-05        |2.015e-05         |l2_reg_2\n",
            "50                |50                |tuner/epochs\n",
            "0                 |17                |tuner/initial_epoch\n",
            "0                 |3                 |tuner/bracket\n",
            "0                 |3                 |tuner/round\n",
            "\n",
            "Epoch 1/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 2703201144930304.0000 - mae: 50158072.0000 - val_loss: 2728000286097408.0000 - val_mae: 50453832.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2702211423404032.0000 - mae: 50181048.0000 - val_loss: 2727991159291904.0000 - val_mae: 50453764.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m  1/300\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 634ms/step - loss: 2829144081563648.0000 - mae: 51248616.0000"
          ]
        }
      ]
    }
  ]
}