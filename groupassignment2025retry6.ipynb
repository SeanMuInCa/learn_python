{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXhd34XKiJsx95obOyC8Cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeanMuInCa/learn_python/blob/master/groupassignment2025retry6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5ad-pXRVXuQ",
        "outputId": "378a84a9-6ce2-4168-a0fc-5543d2583df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial shape of train data: (18741, 35)\n",
            "Shape after dropping missing values: (18740, 35)\n",
            "All force_meas values are positive.\n",
            "Dataset info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 18740 entries, 0 to 18739\n",
            "Data columns (total 35 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   tappingsteelgrade     18740 non-null  object \n",
            " 1   force_meas            18740 non-null  float64\n",
            " 2   speed                 18740 non-null  float64\n",
            " 3   entrytemperature      18740 non-null  float64\n",
            " 4   entrytemperaturebot   18740 non-null  float64\n",
            " 5   entrytemperaturecore  18740 non-null  float64\n",
            " 6   entrytemperaturetop   18740 non-null  float64\n",
            " 7   entrythickness        18740 non-null  float64\n",
            " 8   entrywidth            18740 non-null  float64\n",
            " 9   exitthickness         18740 non-null  float64\n",
            " 10  zeropoint             18740 non-null  float64\n",
            " 11  radius                18740 non-null  float64\n",
            " 12  pctal                 18740 non-null  float64\n",
            " 13  pctb                  18740 non-null  float64\n",
            " 14  pctc                  18740 non-null  float64\n",
            " 15  pctca                 18740 non-null  float64\n",
            " 16  pctco                 18740 non-null  int64  \n",
            " 17  pctcr                 18740 non-null  float64\n",
            " 18  pctcu                 18740 non-null  float64\n",
            " 19  pcth                  18740 non-null  float64\n",
            " 20  pctmg                 18740 non-null  int64  \n",
            " 21  pctmn                 18740 non-null  float64\n",
            " 22  pctmo                 18740 non-null  float64\n",
            " 23  pctn                  18740 non-null  float64\n",
            " 24  pctnb                 18740 non-null  float64\n",
            " 25  pctni                 18740 non-null  float64\n",
            " 26  pcto                  18740 non-null  float64\n",
            " 27  pctp                  18740 non-null  float64\n",
            " 28  pcts                  18740 non-null  float64\n",
            " 29  pctsi                 18740 non-null  float64\n",
            " 30  pctsn                 18740 non-null  float64\n",
            " 31  pctti                 18740 non-null  float64\n",
            " 32  pctv                  18740 non-null  float64\n",
            " 33  pctzr                 18740 non-null  float64\n",
            " 34  fur_line_no           18740 non-null  float64\n",
            "dtypes: float64(32), int64(2), object(1)\n",
            "memory usage: 5.1+ MB\n",
            "None\n",
            "First 5 rows:\n",
            "  tappingsteelgrade   force_meas     speed  entrytemperature  \\\n",
            "0          GL4G71R1  58204143.19  2.634871       1000.661121   \n",
            "1          GL4G71R1  53211949.94  3.947747       1020.846126   \n",
            "2          JV7P1BP6  43513598.87  1.573130        829.078290   \n",
            "3          JT5P31P2  33293124.30  2.979649        736.655045   \n",
            "4          JT5P32P1  68174217.63  3.401038        893.722030   \n",
            "\n",
            "   entrytemperaturebot  entrytemperaturecore  entrytemperaturetop  \\\n",
            "0           939.942163           1034.327117           916.847574   \n",
            "1           991.621746           1044.907925           975.227384   \n",
            "2           786.696648            857.833946           763.533963   \n",
            "3           734.483511            738.987016           730.511453   \n",
            "4           867.859688            913.164933           850.840106   \n",
            "\n",
            "   entrythickness  entrywidth  exitthickness  ...  pctni    pcto    pctp  \\\n",
            "0        0.080232    3.496171       0.065490  ...  0.004  0.0000  0.0153   \n",
            "1        0.039173    3.974567       0.031835  ...  0.004  0.0000  0.0119   \n",
            "2        0.148507    1.912962       0.136898  ...  0.663  0.0025  0.0088   \n",
            "3        0.014806    2.715973       0.013670  ...  0.019  0.0017  0.0150   \n",
            "4        0.048040    3.654979       0.039451  ...  0.010  0.0019  0.0138   \n",
            "\n",
            "     pcts  pctsi  pctsn   pctti    pctv  pctzr  fur_line_no  \n",
            "0  0.0068  0.135  0.001  0.0097  0.0005  0.000          3.0  \n",
            "1  0.0051  0.128  0.001  0.0090  0.0005  0.000          4.0  \n",
            "2  0.0011  0.119  0.001  0.0121  0.0027  0.001          2.0  \n",
            "3  0.0031  0.283  0.002  0.0128  0.0019  0.001          2.0  \n",
            "4  0.0040  0.208  0.002  0.0135  0.0011  0.001          3.0  \n",
            "\n",
            "[5 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preprocessing\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset (ensure 'train.csv' is in the working directory)\n",
        "df = pd.read_csv('train.csv')\n",
        "\n",
        "# Print the initial shape of the dataset\n",
        "print(\"Initial shape of train data:\", df.shape)\n",
        "\n",
        "# Drop any rows with missing values\n",
        "df = df.dropna()\n",
        "print(\"Shape after dropping missing values:\", df.shape)\n",
        "\n",
        "# Check if any force_meas values are non-positive\n",
        "if (df['force_meas'] <= 0).any():\n",
        "    raise ValueError(\"There are non-positive values in force_meas!\")\n",
        "else:\n",
        "    print(\"All force_meas values are positive.\")\n",
        "\n",
        "# Display basic information and the first 5 rows of the dataset\n",
        "print(\"Dataset info:\")\n",
        "print(df.info())\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features by dropping the target column 'force_meas'\n",
        "features = df.drop(columns=['force_meas'])\n",
        "target = df['force_meas']\n",
        "\n",
        "# Convert the categorical column 'tappingsteelgrade' using one-hot encoding\n",
        "if 'tappingsteelgrade' in features.columns:\n",
        "    features = pd.get_dummies(features, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "print(\"Features shape after encoding:\", features.shape)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "print(\"Feature scaling completed. Sample of scaled features:\")\n",
        "print(X_scaled[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjQhx651VgUy",
        "outputId": "76191a96-47b6-4044-da98-a700dc142d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape after encoding: (18740, 56)\n",
            "Feature scaling completed. Sample of scaled features:\n",
            "[[-1.20508792e+00  1.62965337e+00  1.10811970e+00  1.76857847e+00\n",
            "   9.93982742e-01  1.45205203e+00  3.67211646e-01  1.28025798e+00\n",
            "  -2.78962945e-01  1.73111125e+00 -8.51159342e-01 -1.95474602e+00\n",
            "   1.28519135e+00  2.69464606e-01  0.00000000e+00 -4.86948941e-01\n",
            "  -3.98083604e-01  3.60074479e-01  0.00000000e+00 -2.05370375e+00\n",
            "  -8.16150867e-01 -8.30042669e-01 -1.20095624e+00 -3.64509229e-01\n",
            "  -1.47024959e+00  1.43886224e+00  1.66903598e+00 -1.49395116e+00\n",
            "  -7.12974095e-01 -9.57176464e-01 -1.08105134e+00 -1.66510560e+00\n",
            "   4.40596805e-01 -3.93686479e-02  3.15073225e+00 -2.98482242e-01\n",
            "  -6.78989783e-02 -1.63341806e-01 -3.26860225e-02 -1.63364675e-02\n",
            "  -1.26465387e-01 -1.50488065e-01 -3.94794351e-01 -3.45747294e-01\n",
            "  -3.13603578e-01 -2.79844432e-01 -2.27994709e-01 -3.42832211e-02\n",
            "  -1.05942574e-01 -1.46113908e-02 -3.50546631e-02 -1.03312621e-02\n",
            "  -2.75048132e-01 -1.63364675e-02 -3.98847920e-01 -2.53130760e-02]\n",
            " [ 1.84006982e-01  1.91602469e+00  1.89321506e+00  1.90797079e+00\n",
            "   1.91394272e+00 -7.01677167e-02  1.16662492e+00 -1.40059172e-01\n",
            "  -1.20885401e+00  2.68156998e-01 -4.58079270e-01 -1.95474602e+00\n",
            "   9.89673851e-01  8.83140880e-01  0.00000000e+00 -1.24212513e+00\n",
            "  -4.84184047e-01  2.19264752e+00  0.00000000e+00 -2.24020394e+00\n",
            "  -1.09132885e+00 -3.74294184e-01 -1.16175107e+00 -3.64509229e-01\n",
            "  -1.47024959e+00  1.41297245e-01  7.74640848e-01 -1.63198564e+00\n",
            "  -7.12974095e-01 -1.31720791e+00 -1.08105134e+00 -1.66510560e+00\n",
            "   1.33535018e+00 -3.93686479e-02  3.15073225e+00 -2.98482242e-01\n",
            "  -6.78989783e-02 -1.63341806e-01 -3.26860225e-02 -1.63364675e-02\n",
            "  -1.26465387e-01 -1.50488065e-01 -3.94794351e-01 -3.45747294e-01\n",
            "  -3.13603578e-01 -2.79844432e-01 -2.27994709e-01 -3.42832211e-02\n",
            "  -1.05942574e-01 -1.46113908e-02 -3.50546631e-02 -1.03312621e-02\n",
            "  -2.75048132e-01 -1.63364675e-02 -3.98847920e-01 -2.53130760e-02]\n",
            " [-2.32846940e+00 -8.04648764e-01 -1.21992428e+00 -5.56555035e-01\n",
            "  -1.42196187e+00  3.98334306e+00 -2.27837601e+00  4.29386612e+00\n",
            "  -3.06864306e+00 -1.00672645e+00 -1.03366080e+00  1.01999402e+00\n",
            "  -2.16907989e+00  1.32148107e+00  0.00000000e+00  1.76372795e+01\n",
            "   1.35071379e+01 -9.48906264e-01  0.00000000e+00  1.32449289e+00\n",
            "   2.53257571e+01  3.85286624e-01  2.82410717e+00  2.22261106e+01\n",
            "   1.19308127e+00 -1.04177672e+00 -1.32981828e+00 -1.80945853e+00\n",
            "  -7.12974095e-01  2.77217081e-01  1.73245415e+00  6.31734885e-02\n",
            "  -4.54156568e-01 -3.93686479e-02 -3.17386538e-01 -2.98482242e-01\n",
            "  -6.78989783e-02 -1.63341806e-01 -3.26860225e-02 -1.63364675e-02\n",
            "  -1.26465387e-01 -1.50488065e-01 -3.94794351e-01 -3.45747294e-01\n",
            "  -3.13603578e-01 -2.79844432e-01 -2.27994709e-01 -3.42832211e-02\n",
            "  -1.05942574e-01 -1.46113908e-02 -3.50546631e-02 -1.03312621e-02\n",
            "  -2.75048132e-01 -1.63364675e-02 -3.98847920e-01  3.95052739e+01]\n",
            " [-8.40294039e-01 -2.11588779e+00 -2.01312518e+00 -2.12225290e+00\n",
            "  -1.94233677e+00 -9.73571076e-01 -9.36520954e-01 -9.06680031e-01\n",
            "   9.99640278e-01  8.05443879e-01 -2.19423511e-01  1.01999402e+00\n",
            "  -6.35672409e-01 -5.19547745e-01  0.00000000e+00  1.40099152e+00\n",
            "   7.54688297e-02  3.60074479e-01  0.00000000e+00  2.73309996e-01\n",
            "   9.38306983e-03 -2.22378023e-01  6.80891590e-01  1.49693044e-01\n",
            "   3.40815393e-01  1.32437121e+00 -2.77588715e-01  1.42449198e+00\n",
            "   2.79680774e-01  6.37248532e-01  7.09361247e-01  6.31734885e-02\n",
            "  -4.54156568e-01 -3.93686479e-02 -3.17386538e-01 -2.98482242e-01\n",
            "  -6.78989783e-02 -1.63341806e-01 -3.26860225e-02 -1.63364675e-02\n",
            "  -1.26465387e-01 -1.50488065e-01 -3.94794351e-01  2.89228583e+00\n",
            "  -3.13603578e-01 -2.79844432e-01 -2.27994709e-01 -3.42832211e-02\n",
            "  -1.05942574e-01 -1.46113908e-02 -3.50546631e-02 -1.03312621e-02\n",
            "  -2.75048132e-01 -1.63364675e-02 -3.98847920e-01 -2.53130760e-02]\n",
            " [-3.94441837e-01  1.12473261e-01  1.30718505e-02  1.72379291e-01\n",
            "  -4.61753071e-02  2.58546947e-01  6.32583450e-01  1.81344477e-01\n",
            "  -1.62726389e-01 -1.79429845e+00  2.71926579e-01  2.84140057e-02\n",
            "  -1.38760027e+00 -8.70219902e-01  0.00000000e+00  1.42364546e-01\n",
            "  -2.25882719e-01 -6.87110116e-01  0.00000000e+00  4.72526109e-01\n",
            "   2.84561049e-01 -1.89345580e+00  4.32592224e-01 -1.58828320e-01\n",
            "   5.53881862e-01  8.66407097e-01  1.95914589e-01 -5.44488017e-02\n",
            "   2.79680774e-01  9.97279983e-01 -3.13731658e-01  6.31734885e-02\n",
            "   4.40596805e-01 -3.93686479e-02 -3.17386538e-01 -2.98482242e-01\n",
            "  -6.78989783e-02 -1.63341806e-01 -3.26860225e-02 -1.63364675e-02\n",
            "  -1.26465387e-01 -1.50488065e-01 -3.94794351e-01 -3.45747294e-01\n",
            "  -3.13603578e-01  3.57341395e+00 -2.27994709e-01 -3.42832211e-02\n",
            "  -1.05942574e-01 -1.46113908e-02 -3.50546631e-02 -1.03312621e-02\n",
            "  -2.75048132e-01 -1.63364675e-02 -3.98847920e-01 -2.53130760e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# Split the scaled features and target into training and validation sets\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, df['force_meas'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define 5-fold cross-validation\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr_cv_scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
        "print(\"Linear Regression CV MAE: {:.4f}\".format(-np.mean(lr_cv_scores)))\n",
        "\n",
        "# -------------------------------\n",
        "# Decision Tree Regressor with Grid Search\n",
        "# -------------------------------\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "dt_param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "dt_grid = GridSearchCV(dt, dt_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "dt_grid.fit(X_train, y_train)\n",
        "print(\"Decision Tree best params:\", dt_grid.best_params_)\n",
        "print(\"Decision Tree CV MAE: {:.4f}\".format(-dt_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Random Forest Regressor with Grid Search\n",
        "# -------------------------------\n",
        "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5]\n",
        "}\n",
        "rf_grid = GridSearchCV(rf, rf_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "print(\"Random Forest best params:\", rf_grid.best_params_)\n",
        "print(\"Random Forest CV MAE: {:.4f}\".format(-rf_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Gradient Boosting Regressor with Grid Search\n",
        "# -------------------------------\n",
        "gb = GradientBoostingRegressor(random_state=42)\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 5]\n",
        "}\n",
        "gb_grid = GridSearchCV(gb, gb_param_grid, cv=cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "gb_grid.fit(X_train, y_train)\n",
        "print(\"Gradient Boosting best params:\", gb_grid.best_params_)\n",
        "print(\"Gradient Boosting CV MAE: {:.4f}\".format(-gb_grid.best_score_))\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate each model on the validation set\n",
        "# -------------------------------\n",
        "models = {\n",
        "    'Linear Regression': lr,\n",
        "    'Decision Tree': dt_grid.best_estimator_,\n",
        "    'Random Forest': rf_grid.best_estimator_,\n",
        "    'Gradient Boosting': gb_grid.best_estimator_\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "    print(f\"{name} Validation MAE: {mae:.4f}\")\n",
        "    print(f\"{name} Validation RMSE: {rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BW-hd52Vg4r",
        "outputId": "56dbf576-dd0e-41d1-b619-eb89373da9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression CV MAE: 5530930.6036\n",
            "Decision Tree best params: {'max_depth': 10, 'min_samples_split': 10}\n",
            "Decision Tree CV MAE: 6586591.3970\n",
            "Random Forest best params: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Random Forest CV MAE: 5102189.1475\n",
            "Gradient Boosting best params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
            "Gradient Boosting CV MAE: 4965115.6889\n",
            "Linear Regression Validation MAE: 5617899.1469\n",
            "Linear Regression Validation RMSE: 7308625.1658\n",
            "Decision Tree Validation MAE: 6583854.2908\n",
            "Decision Tree Validation RMSE: 8584133.7299\n",
            "Random Forest Validation MAE: 4920905.1283\n",
            "Random Forest Validation RMSE: 6366341.4165\n",
            "Gradient Boosting Validation MAE: 4968688.1502\n",
            "Gradient Boosting Validation RMSE: 6349974.3617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model_names = []\n",
        "mae_scores = []\n",
        "rmse_scores = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    model_names.append(name)\n",
        "    mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
        "    rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# -------- MAE  --------\n",
        "plt.subplot(1, 2, 1)\n",
        "bars1 = plt.bar(model_names, mae_scores)\n",
        "plt.title(\"Model Comparison - MAE\")\n",
        "plt.ylabel(\"Mean Absolute Error\")\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "for bar in bars1:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "# -------- RMSE  --------\n",
        "plt.subplot(1, 2, 2)\n",
        "bars2 = plt.bar(model_names, rmse_scores)\n",
        "plt.title(\"Model Comparison - RMSE\")\n",
        "plt.ylabel(\"Root Mean Squared Error\")\n",
        "plt.xticks(rotation=15)\n",
        "\n",
        "for bar in bars2:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "XW8BkLmxVj6L",
        "outputId": "57b8d9f0-6411-4f51-b338-8290e36cde18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the fitted scaler for later use\n",
        "with open('scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)  # Save the scaler object\n",
        "\n",
        "# Save the best Random Forest model (obtained from GridSearchCV)\n",
        "with open('best_rf_model.pkl', 'wb') as f:\n",
        "    pickle.dump(rf_grid.best_estimator_, f)  # Save the best RF model\n",
        "\n",
        "# Save the best Gradient Boosting model (obtained from GridSearchCV)\n",
        "with open('best_gb_model.pkl', 'wb') as f:\n",
        "    pickle.dump(gb_grid.best_estimator_, f)  # Save the best GB model\n",
        "\n",
        "# Save the training columns (features names) for later use\n",
        "training_columns = features.columns.tolist()  # 'features'  DataFrame\n",
        "with open('training_columns.pkl', 'wb') as f:\n",
        "    pickle.dump(training_columns, f)\n",
        "print(\"Training columns have been saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p9XkJk6Vkjq",
        "outputId": "34051154-e069-4b3e-d05b-f4fb4a28579d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training columns have been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Step 11: Final Evaluation on Test Data\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load test data (ensure test.csv is in your working directory)\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Drop missing values if any (you may adjust based on your preprocessing strategy)\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# For test data, note: do not use the 'force_pre' column for prediction, it's the baseline.\n",
        "# We assume test_df contains 'force_pre' and other features including 'tappingsteelgrade'\n",
        "# Also assume that 'force_meas' is present for evaluation\n",
        "# If not, adjust accordingly (for final evaluation, force_meas is needed)\n",
        "\n",
        "# Separate baseline predictions for later comparison\n",
        "baseline_force_pre = test_df['force_pre']\n",
        "\n",
        "# Separate target variable for evaluation (force_meas)\n",
        "y_test = test_df['force_meas']\n",
        "\n",
        "# ===============================\n",
        "# Preprocessing: Feature Selection & Encoding\n",
        "# ===============================\n",
        "# Exclude the target 'force_meas' and baseline column 'force_pre'\n",
        "features_test = test_df.drop(columns=['force_meas', 'force_pre'])\n",
        "\n",
        "# One-hot encode the categorical column 'tappingsteelgrade' (using the same columns as during training)\n",
        "features_test = pd.get_dummies(features_test, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "# IMPORTANT: Align the test features with the training features.\n",
        "# Load the fitted scaler (assumes scaler.pkl was saved during training)\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# To ensure that test data has the same feature columns as training data,\n",
        "# we reindex the test DataFrame to match training columns.\n",
        "# Assume that 'training_columns' was saved during training.\n",
        "with open('training_columns.pkl', 'rb') as f:\n",
        "    training_columns = pickle.load(f)\n",
        "\n",
        "# Reindex the test features DataFrame; missing columns will be filled with zeros.\n",
        "features_test = features_test.reindex(columns=training_columns, fill_value=0)\n",
        "\n",
        "# Scale the test features using the saved scaler\n",
        "X_test_scaled = scaler.transform(features_test)\n",
        "\n",
        "# ===============================\n",
        "# Load the saved models\n",
        "# ===============================\n",
        "# Load best Random Forest model\n",
        "with open('best_rf_model.pkl', 'rb') as f:\n",
        "    best_rf_model = pickle.load(f)\n",
        "\n",
        "# Load best Gradient Boosting model\n",
        "with open('best_gb_model.pkl', 'rb') as f:\n",
        "    best_gb_model = pickle.load(f)\n",
        "\n",
        "# ===============================\n",
        "# Evaluate Models on Test Data\n",
        "# ===============================\n",
        "# Evaluate Random Forest model\n",
        "start_time = time.time()\n",
        "rf_predictions = best_rf_model.predict(X_test_scaled)\n",
        "rf_runtime = (time.time() - start_time)\n",
        "\n",
        "rf_mae = mean_absolute_error(y_test, rf_predictions)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Evaluate Gradient Boosting model\n",
        "start_time = time.time()\n",
        "gb_predictions = best_gb_model.predict(X_test_scaled)\n",
        "gb_runtime = (time.time() - start_time)\n",
        "\n",
        "gb_mae = mean_absolute_error(y_test, gb_predictions)\n",
        "gb_rmse = np.sqrt(mean_squared_error(y_test, gb_predictions))\n",
        "gb_mse = mean_squared_error(y_test, gb_predictions)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Final Evaluation Metrics:\")\n",
        "print(\"Random Forest - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    rf_mae, rf_rmse, rf_mse, rf_runtime))\n",
        "print(\"Gradient Boosting - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}, Avg Runtime per sample: {:.6f} sec\".format(\n",
        "    gb_mae, gb_rmse, gb_mse, gb_runtime))\n",
        "\n",
        "# Compare with baseline force_pre (if available)\n",
        "# Calculate baseline evaluation metrics using force_pre column\n",
        "baseline_mae = mean_absolute_error(y_test, baseline_force_pre)\n",
        "baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_force_pre))\n",
        "baseline_mse = mean_squared_error(y_test, baseline_force_pre)\n",
        "\n",
        "print(\"Baseline (force_pre) - MAE: {:.4f}, RMSE: {:.4f}, MSE: {:.4f}\".format(\n",
        "    baseline_mae, baseline_rmse, baseline_mse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyqDdpbPVmcD",
        "outputId": "016aa49d-c3ef-401d-8216-3587b1f113ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation Metrics:\n",
            "Random Forest - MAE: 4845642.3052, RMSE: 6330958.0330, MSE: 40081029614984.8828, Avg Runtime per sample: 0.402613 sec\n",
            "Gradient Boosting - MAE: 4892831.9861, RMSE: 6272410.9957, MSE: 39343139699307.8750, Avg Runtime per sample: 0.052553 sec\n",
            "Baseline (force_pre) - MAE: 2898684.1133, RMSE: 3775600.3207, MSE: 14255157781834.8516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Keras Tuner if not already installed\n",
        "!pip install keras-tuner\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "\n",
        "# Define a model-building function for the tuner\n",
        "def build_model(hp):\n",
        "    \"\"\"\n",
        "    Build a DNN model with hyperparameters to tune.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Input layer using an Input layer is preferred\n",
        "    model.add(keras.Input(shape=(X_train.shape[1],)))  # X_train\n",
        "\n",
        "    # Tune the number of layers (at least 1 layer)\n",
        "    num_layers = hp.Int('num_layers', 1, 3, default=2)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        # Tune the number of units in this layer\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=256, step=32, default=64)\n",
        "        # Tune dropout rate for this layer\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1, default=0.2)\n",
        "        # Tune L2 regularization factor\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', 1e-5, 1e-3, sampling='LOG', default=1e-4)\n",
        "\n",
        "        model.add(layers.Dense(units, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "        model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Tune learning rate for the optimizer\n",
        "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG', default=1e-3)\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Set up the tuner: Here we use Hyperband search algorithm\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_mae',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='dnn_tuner_dir',\n",
        "    project_name='force_prediction_tuning'\n",
        ")\n",
        "\n",
        "# Optional: Early stopping callback to stop training if no improvement\n",
        "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Perform hyperparameter search\n",
        "tuner.search(X_train, y_train,\n",
        "             epochs=50,\n",
        "             validation_split=0.2,\n",
        "             callbacks=[stop_early],\n",
        "             verbose=1)\n",
        "\n",
        "# Retrieve the best hyperparameters and build the best model\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model and train it\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "history = best_model.fit(X_train, y_train,\n",
        "                         epochs=50,\n",
        "                         validation_split=0.2,\n",
        "                         callbacks=[stop_early],\n",
        "                         verbose=1)\n",
        "\n",
        "\n",
        "best_model.save('optimized_dnn_model.h5')\n",
        "print(\"Optimized DNN model has been saved successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--vuM5JEVoEi",
        "outputId": "631b60d4-1420-42ab-abf8-c35c3432418d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 90 Complete [00h 02m 47s]\n",
            "val_mae: 50106732.0\n",
            "\n",
            "Best val_mae So Far: 31083054.0\n",
            "Total elapsed time: 00h 53m 26s\n",
            "Best hyperparameters found: {'num_layers': 3, 'units_0': 192, 'dropout_0': 0.1, 'l2_reg_0': 2.0316361619530612e-05, 'units_1': 192, 'dropout_1': 0.4, 'l2_reg_1': 2.4454459675378995e-05, 'learning_rate': 0.009673431123937811, 'units_2': 256, 'dropout_2': 0.1, 'l2_reg_2': 1.6047585761644394e-05, 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0046'}\n",
            "Epoch 1/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 2695280419930112.0000 - mae: 50116676.0000 - val_loss: 2696109080182784.0000 - val_mae: 50144500.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2673205093335040.0000 - mae: 49928132.0000 - val_loss: 2689138616696832.0000 - val_mae: 50086568.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2707859573833728.0000 - mae: 50254780.0000 - val_loss: 2681297080156160.0000 - val_mae: 50025680.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2696580989714432.0000 - mae: 50179116.0000 - val_loss: 2665048682004480.0000 - val_mae: 49888680.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2660816797040640.0000 - mae: 49828300.0000 - val_loss: 2654692978982912.0000 - val_mae: 49812728.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2651088964550656.0000 - mae: 49778188.0000 - val_loss: 2637636321673216.0000 - val_mae: 49670444.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2648557081329664.0000 - mae: 49768124.0000 - val_loss: 2614127281307648.0000 - val_mae: 49472412.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2620690796642304.0000 - mae: 49528772.0000 - val_loss: 2589913060999168.0000 - val_mae: 49265624.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2580878865727488.0000 - mae: 49184796.0000 - val_loss: 2567426222850048.0000 - val_mae: 49083352.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 2556407484252160.0000 - mae: 48978748.0000 - val_loss: 2558254051753984.0000 - val_mae: 49028432.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2557584842162176.0000 - mae: 49023708.0000 - val_loss: 2507106359967744.0000 - val_mae: 48560008.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2540254380687360.0000 - mae: 48876216.0000 - val_loss: 2492145378263040.0000 - val_mae: 48460392.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2473969345101824.0000 - mae: 48294272.0000 - val_loss: 2467259532443648.0000 - val_mae: 48242456.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2462450511249408.0000 - mae: 48212964.0000 - val_loss: 2430766805942272.0000 - val_mae: 47918288.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 2429103579856896.0000 - mae: 47913836.0000 - val_loss: 2393375625969664.0000 - val_mae: 47581656.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2400252405481472.0000 - mae: 47659412.0000 - val_loss: 2363141472124928.0000 - val_mae: 47293392.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 2369860411588608.0000 - mae: 47381084.0000 - val_loss: 2317458857787392.0000 - val_mae: 46905628.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2315536591486976.0000 - mae: 46877692.0000 - val_loss: 2312434819792896.0000 - val_mae: 46922108.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2308327220445184.0000 - mae: 46846144.0000 - val_loss: 2263937391263744.0000 - val_mae: 46450804.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2257567417892864.0000 - mae: 46372756.0000 - val_loss: 2239007924682752.0000 - val_mae: 46235788.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 2216815828664320.0000 - mae: 46005156.0000 - val_loss: 2175423752437760.0000 - val_mae: 45603140.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 2190846577344512.0000 - mae: 45774584.0000 - val_loss: 2170171946958848.0000 - val_mae: 45583052.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2157027065331712.0000 - mae: 45468360.0000 - val_loss: 2149793669316608.0000 - val_mae: 45437368.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2110132062257152.0000 - mae: 45003004.0000 - val_loss: 2050410005135360.0000 - val_mae: 44445412.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 2068011854856192.0000 - mae: 44599112.0000 - val_loss: 2062462488674304.0000 - val_mae: 44610764.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2050390946217984.0000 - mae: 44431360.0000 - val_loss: 1947938125250560.0000 - val_mae: 43371200.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1982887347879936.0000 - mae: 43757552.0000 - val_loss: 1939710310088704.0000 - val_mae: 43335972.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1945067610701824.0000 - mae: 43371280.0000 - val_loss: 1931709457104896.0000 - val_mae: 43307296.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1898626531983360.0000 - mae: 42882436.0000 - val_loss: 1875658523279360.0000 - val_mae: 42706136.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 1860995907584000.0000 - mae: 42512664.0000 - val_loss: 1741101627080704.0000 - val_mae: 41130892.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1816299390894080.0000 - mae: 42034240.0000 - val_loss: 1768265147744256.0000 - val_mae: 41537932.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1785070146813952.0000 - mae: 41697116.0000 - val_loss: 1724130600681472.0000 - val_mae: 41097732.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1718924697665536.0000 - mae: 40955548.0000 - val_loss: 1675752760147968.0000 - val_mae: 40548152.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1674196371374080.0000 - mae: 40444824.0000 - val_loss: 1621876153516032.0000 - val_mae: 39906800.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1625748469186560.0000 - mae: 39892868.0000 - val_loss: 1665362831605760.0000 - val_mae: 40458104.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1587681637171200.0000 - mae: 39447860.0000 - val_loss: 1496381638311936.0000 - val_mae: 38373572.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 1544254014881792.0000 - mae: 38915456.0000 - val_loss: 1415993674956800.0000 - val_mae: 37391380.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1483301718065152.0000 - mae: 38150884.0000 - val_loss: 1456437905588224.0000 - val_mae: 37937340.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 1433206695919616.0000 - mae: 37511780.0000 - val_loss: 1391751000489984.0000 - val_mae: 37057984.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 1404132854333440.0000 - mae: 37145788.0000 - val_loss: 1493049012125696.0000 - val_mae: 38392652.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1363614267080704.0000 - mae: 36605256.0000 - val_loss: 1305847359602688.0000 - val_mae: 35948976.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1313546491133952.0000 - mae: 35940948.0000 - val_loss: 1300802115207168.0000 - val_mae: 35892064.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1265578014670848.0000 - mae: 35282680.0000 - val_loss: 1221229256114176.0000 - val_mae: 34719956.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1217311037980672.0000 - mae: 34576784.0000 - val_loss: 1229208231608320.0000 - val_mae: 34876140.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 1182226926534656.0000 - mae: 34058860.0000 - val_loss: 1122878330241024.0000 - val_mae: 33319768.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 1128570973847552.0000 - mae: 33281110.0000 - val_loss: 1158756138221568.0000 - val_mae: 33857252.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 1092574047633408.0000 - mae: 32734378.0000 - val_loss: 1023591369932800.0000 - val_mae: 31811990.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 1049453179961344.0000 - mae: 32052142.0000 - val_loss: 966073503449088.0000 - val_mae: 30872798.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 1010064705978368.0000 - mae: 31431988.0000 - val_loss: 969510953680896.0000 - val_mae: 30946718.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m525/525\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 957543429963776.0000 - mae: 30620046.0000 - val_loss: 903072171687936.0000 - val_mae: 29850002.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized DNN model has been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "scaler_dnn_log = StandardScaler()\n",
        "X_train_scaled = scaler_dnn_log.fit_transform(X_train)\n",
        "X_val_scaled   = scaler_dnn_log.transform(X_val)\n",
        "# scaler\n",
        "with open('scaler_dnn_log.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler_dnn_log, f)\n",
        "\n",
        "\n",
        "# Assume that X_train, y_train, X_val, and y_val are already preprocessed\n",
        "# and that y_train and y_val have been log-transformed\n",
        "y_train_log = np.log(y_train)\n",
        "y_val_log = np.log(y_val)\n",
        "\n",
        "# Get the number of features from X_train\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "# Define the model-building function for the tuner\n",
        "def build_advanced_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(input_dim,)))\n",
        "\n",
        "    # Loop to add multiple hidden layers (between 4 and 6 layers)\n",
        "    for i in range(hp.Int('num_layers', 4, 6)):\n",
        "        # Define the number of units in the layer (range: 32 to 512, step size: 32)\n",
        "        units = hp.Int(f'units_{i}', min_value=32, max_value=512, step=32)\n",
        "        # Choose the activation function: ReLU or LeakyReLU\n",
        "        activation_choice = hp.Choice(f'activation_{i}', values=['relu', 'leaky_relu'])\n",
        "        # Define dropout rate (from 0.0 to 0.5)\n",
        "        dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Define L2 regularization factor (using log sampling)\n",
        "        l2_reg = hp.Float(f'l2_reg_{i}', min_value=1e-6, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "        # Add a Dense layer with L2 regularization\n",
        "        model.add(layers.Dense(units, kernel_regularizer=regularizers.l2(l2_reg)))\n",
        "\n",
        "        # Add the chosen activation function\n",
        "        if activation_choice == 'leaky_relu':\n",
        "            model.add(layers.LeakyReLU(alpha=0.1))\n",
        "        else:\n",
        "            model.add(layers.Activation('relu'))\n",
        "\n",
        "        # Add a Dropout layer\n",
        "        model.add(layers.Dropout(dropout_rate))\n",
        "\n",
        "        # Optionally add Batch Normalization after each layer\n",
        "        if hp.Boolean(f'batchnorm_{i}'):\n",
        "            model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Output layer for predicting log(force_meas)\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    # Define the learning rate\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model with MSE loss and MAE metric\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# Use Keras Tuner for hyperparameter tuning with RandomSearch\n",
        "tuner = kt.RandomSearch(\n",
        "    build_advanced_model,\n",
        "    objective='val_mae',\n",
        "    max_trials=30,\n",
        "    executions_per_trial=1,\n",
        "    directory='advanced_dnn_tuning',\n",
        "    project_name='rolling_force_advanced'\n",
        ")\n",
        "\n",
        "# Start the hyperparameter search\n",
        "tuner.search(X_train, y_train_log, epochs=50, validation_data=(X_val, y_val_log))\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"Best hyperparameters found:\", best_hp.values)\n",
        "\n",
        "# Build the best model using the best hyperparameters\n",
        "best_advanced_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "# Define callbacks for learning rate reduction and early stopping\n",
        "lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "early_stopper = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the best model\n",
        "history = best_advanced_model.fit(X_train, y_train_log,\n",
        "                                  epochs=100,\n",
        "                                  validation_data=(X_val, y_val_log),\n",
        "                                  callbacks=[lr_reducer, early_stopper])\n",
        "\n",
        "# Save the optimized model (using HDF5 format; you can also use the native Keras format)\n",
        "best_advanced_model.save(\"optimized_dnn_model_advanced_log.h5\")\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluate the optimized model on the validation set\n",
        "# -------------------------------\n",
        "# Predict log-transformed values on validation set and revert the transformation\n",
        "y_val_pred_log = best_advanced_model.predict(X_val).flatten()\n",
        "y_val_pred = np.exp(y_val_pred_log)  # Reverse log transformation\n",
        "y_val_orig = np.exp(y_val_log)\n",
        "\n",
        "# Calculate evaluation metrics on the original scale\n",
        "val_mae = mean_absolute_error(y_val_orig, y_val_pred)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val_orig, y_val_pred))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation MAE: {:.4f}\".format(val_mae))\n",
        "print(\"Advanced Optimized DNN (with log transformation) Validation RMSE: {:.4f}\".format(val_rmse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8LzRFDsVpuB",
        "outputId": "815d0c81-9ece-41d0-df17-b62c7c18d814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 03m 35s]\n",
            "val_mae: 0.1309540569782257\n",
            "\n",
            "Best val_mae So Far: 0.06579969823360443\n",
            "Total elapsed time: 03h 58m 35s\n",
            "Best hyperparameters found: {'num_layers': 5, 'units_0': 512, 'activation_0': 'leaky_relu', 'dropout_0': 0.0, 'l2_reg_0': 7.995176857691583e-06, 'batchnorm_0': False, 'units_1': 128, 'activation_1': 'relu', 'dropout_1': 0.4, 'l2_reg_1': 0.00027489569105716184, 'batchnorm_1': False, 'units_2': 416, 'activation_2': 'leaky_relu', 'dropout_2': 0.30000000000000004, 'l2_reg_2': 1.1136052928650208e-05, 'batchnorm_2': True, 'units_3': 224, 'activation_3': 'relu', 'dropout_3': 0.0, 'l2_reg_3': 2.1960403473513426e-05, 'batchnorm_3': True, 'learning_rate': 0.0016181139257847523, 'units_4': 96, 'activation_4': 'leaky_relu', 'dropout_4': 0.30000000000000004, 'l2_reg_4': 0.000665443604336913, 'batchnorm_4': True, 'units_5': 192, 'activation_5': 'leaky_relu', 'dropout_5': 0.30000000000000004, 'l2_reg_5': 0.00035869504344532955, 'batchnorm_5': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 134.8878 - mae: 9.3854 - val_loss: 0.3252 - val_mae: 0.2781 - learning_rate: 0.0016\n",
            "Epoch 2/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 2.8225 - mae: 1.0543 - val_loss: 0.2560 - val_mae: 0.2209 - learning_rate: 0.0016\n",
            "Epoch 3/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 1.2530 - mae: 0.6654 - val_loss: 0.2164 - val_mae: 0.1779 - learning_rate: 0.0016\n",
            "Epoch 4/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.6452 - mae: 0.4388 - val_loss: 0.1885 - val_mae: 0.1571 - learning_rate: 0.0016\n",
            "Epoch 5/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.3566 - mae: 0.3085 - val_loss: 0.1658 - val_mae: 0.1508 - learning_rate: 0.0016\n",
            "Epoch 6/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.2135 - mae: 0.2195 - val_loss: 0.1395 - val_mae: 0.1414 - learning_rate: 0.0016\n",
            "Epoch 7/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.1553 - mae: 0.1774 - val_loss: 0.1163 - val_mae: 0.1411 - learning_rate: 0.0016\n",
            "Epoch 8/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.1180 - mae: 0.1545 - val_loss: 0.0949 - val_mae: 0.1434 - learning_rate: 0.0016\n",
            "Epoch 9/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0918 - mae: 0.1462 - val_loss: 0.0720 - val_mae: 0.1353 - learning_rate: 0.0016\n",
            "Epoch 10/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0729 - mae: 0.1443 - val_loss: 0.0594 - val_mae: 0.1375 - learning_rate: 0.0016\n",
            "Epoch 11/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0568 - mae: 0.1365 - val_loss: 0.0493 - val_mae: 0.1347 - learning_rate: 0.0016\n",
            "Epoch 12/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0474 - mae: 0.1342 - val_loss: 0.0412 - val_mae: 0.1215 - learning_rate: 0.0016\n",
            "Epoch 13/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0424 - mae: 0.1308 - val_loss: 0.0392 - val_mae: 0.1305 - learning_rate: 0.0016\n",
            "Epoch 14/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0388 - mae: 0.1281 - val_loss: 0.0340 - val_mae: 0.1200 - learning_rate: 0.0016\n",
            "Epoch 15/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0365 - mae: 0.1257 - val_loss: 0.0269 - val_mae: 0.1020 - learning_rate: 0.0016\n",
            "Epoch 16/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0331 - mae: 0.1204 - val_loss: 0.0335 - val_mae: 0.1087 - learning_rate: 0.0016\n",
            "Epoch 17/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1153 - val_loss: 0.0322 - val_mae: 0.1272 - learning_rate: 0.0016\n",
            "Epoch 18/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0280 - mae: 0.1130 - val_loss: 0.0255 - val_mae: 0.0981 - learning_rate: 0.0016\n",
            "Epoch 19/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0261 - mae: 0.1096 - val_loss: 0.0212 - val_mae: 0.0928 - learning_rate: 0.0016\n",
            "Epoch 20/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0255 - mae: 0.1105 - val_loss: 0.0210 - val_mae: 0.0850 - learning_rate: 0.0016\n",
            "Epoch 21/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0245 - mae: 0.1075 - val_loss: 0.0290 - val_mae: 0.1073 - learning_rate: 0.0016\n",
            "Epoch 22/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.0223 - mae: 0.1029 - val_loss: 0.0318 - val_mae: 0.0874 - learning_rate: 0.0016\n",
            "Epoch 23/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0229 - mae: 0.1020 - val_loss: 0.0282 - val_mae: 0.1133 - learning_rate: 0.0016\n",
            "Epoch 24/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0246 - mae: 0.1080 - val_loss: 0.0237 - val_mae: 0.1069 - learning_rate: 0.0016\n",
            "Epoch 25/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0221 - mae: 0.1006 - val_loss: 0.0164 - val_mae: 0.0804 - learning_rate: 0.0016\n",
            "Epoch 26/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0216 - mae: 0.0991 - val_loss: 0.0156 - val_mae: 0.0768 - learning_rate: 0.0016\n",
            "Epoch 27/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0227 - mae: 0.1025 - val_loss: 0.0189 - val_mae: 0.0916 - learning_rate: 0.0016\n",
            "Epoch 28/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0231 - mae: 0.1032 - val_loss: 0.0272 - val_mae: 0.1144 - learning_rate: 0.0016\n",
            "Epoch 29/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0220 - mae: 0.1007 - val_loss: 0.0219 - val_mae: 0.0880 - learning_rate: 0.0016\n",
            "Epoch 30/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0228 - mae: 0.1016 - val_loss: 0.0260 - val_mae: 0.1102 - learning_rate: 0.0016\n",
            "Epoch 31/100\n",
            "\u001b[1m651/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0222 - mae: 0.0998\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0008090569754131138.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0222 - mae: 0.0998 - val_loss: 0.0210 - val_mae: 0.1016 - learning_rate: 0.0016\n",
            "Epoch 32/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0176 - mae: 0.0892 - val_loss: 0.0189 - val_mae: 0.0972 - learning_rate: 8.0906e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - loss: 0.0163 - mae: 0.0872 - val_loss: 0.0117 - val_mae: 0.0688 - learning_rate: 8.0906e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0160 - mae: 0.0868 - val_loss: 0.0142 - val_mae: 0.0750 - learning_rate: 8.0906e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0154 - mae: 0.0845 - val_loss: 0.0209 - val_mae: 0.1063 - learning_rate: 8.0906e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0155 - mae: 0.0853 - val_loss: 0.0377 - val_mae: 0.1508 - learning_rate: 8.0906e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0176 - mae: 0.0928 - val_loss: 0.0120 - val_mae: 0.0718 - learning_rate: 8.0906e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m653/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0153 - mae: 0.0855\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0004045284877065569.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0153 - mae: 0.0856 - val_loss: 0.0156 - val_mae: 0.0864 - learning_rate: 8.0906e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0132 - mae: 0.0781 - val_loss: 0.0129 - val_mae: 0.0836 - learning_rate: 4.0453e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0124 - mae: 0.0772 - val_loss: 0.0144 - val_mae: 0.0894 - learning_rate: 4.0453e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0129 - mae: 0.0789 - val_loss: 0.0091 - val_mae: 0.0622 - learning_rate: 4.0453e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0123 - mae: 0.0768 - val_loss: 0.0094 - val_mae: 0.0650 - learning_rate: 4.0453e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0121 - mae: 0.0764 - val_loss: 0.0085 - val_mae: 0.0593 - learning_rate: 4.0453e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0131 - mae: 0.0810 - val_loss: 0.0084 - val_mae: 0.0556 - learning_rate: 4.0453e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0117 - mae: 0.0742 - val_loss: 0.0094 - val_mae: 0.0640 - learning_rate: 4.0453e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0124 - mae: 0.0775 - val_loss: 0.0136 - val_mae: 0.0844 - learning_rate: 4.0453e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0122 - mae: 0.0777 - val_loss: 0.0098 - val_mae: 0.0677 - learning_rate: 4.0453e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0118 - mae: 0.0754 - val_loss: 0.0075 - val_mae: 0.0549 - learning_rate: 4.0453e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0116 - mae: 0.0749 - val_loss: 0.0080 - val_mae: 0.0562 - learning_rate: 4.0453e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0115 - mae: 0.0750 - val_loss: 0.0083 - val_mae: 0.0584 - learning_rate: 4.0453e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0119 - mae: 0.0765 - val_loss: 0.0075 - val_mae: 0.0535 - learning_rate: 4.0453e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0114 - mae: 0.0742 - val_loss: 0.0115 - val_mae: 0.0716 - learning_rate: 4.0453e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m650/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0120 - mae: 0.0765\n",
            "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.00020226424385327846.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - loss: 0.0120 - mae: 0.0765 - val_loss: 0.0097 - val_mae: 0.0696 - learning_rate: 4.0453e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0107 - mae: 0.0723 - val_loss: 0.0079 - val_mae: 0.0607 - learning_rate: 2.0226e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - loss: 0.0102 - mae: 0.0696 - val_loss: 0.0080 - val_mae: 0.0599 - learning_rate: 2.0226e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - loss: 0.0102 - mae: 0.0706 - val_loss: 0.0071 - val_mae: 0.0537 - learning_rate: 2.0226e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0094 - mae: 0.0674 - val_loss: 0.0063 - val_mae: 0.0500 - learning_rate: 2.0226e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0102 - mae: 0.0712 - val_loss: 0.0069 - val_mae: 0.0557 - learning_rate: 2.0226e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - loss: 0.0101 - mae: 0.0705 - val_loss: 0.0075 - val_mae: 0.0596 - learning_rate: 2.0226e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0099 - mae: 0.0702 - val_loss: 0.0061 - val_mae: 0.0496 - learning_rate: 2.0226e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0096 - mae: 0.0687 - val_loss: 0.0072 - val_mae: 0.0570 - learning_rate: 2.0226e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.0093 - mae: 0.0672 - val_loss: 0.0058 - val_mae: 0.0485 - learning_rate: 2.0226e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.0104 - mae: 0.0721 - val_loss: 0.0068 - val_mae: 0.0554 - learning_rate: 2.0226e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0095 - mae: 0.0685 - val_loss: 0.0073 - val_mae: 0.0558 - learning_rate: 2.0226e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0095 - mae: 0.0685 - val_loss: 0.0057 - val_mae: 0.0473 - learning_rate: 2.0226e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0094 - mae: 0.0680 - val_loss: 0.0070 - val_mae: 0.0532 - learning_rate: 2.0226e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m655/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0100 - mae: 0.0703\n",
            "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00010113212192663923.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0100 - mae: 0.0703 - val_loss: 0.0087 - val_mae: 0.0648 - learning_rate: 2.0226e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0087 - mae: 0.0651 - val_loss: 0.0057 - val_mae: 0.0472 - learning_rate: 1.0113e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0087 - mae: 0.0653 - val_loss: 0.0062 - val_mae: 0.0528 - learning_rate: 1.0113e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0088 - mae: 0.0654 - val_loss: 0.0067 - val_mae: 0.0548 - learning_rate: 1.0113e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.0087 - mae: 0.0659 - val_loss: 0.0063 - val_mae: 0.0522 - learning_rate: 1.0113e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m653/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0086 - mae: 0.0659\n",
            "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.0566060963319615e-05.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0086 - mae: 0.0659 - val_loss: 0.0062 - val_mae: 0.0521 - learning_rate: 1.0113e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0080 - mae: 0.0621 - val_loss: 0.0062 - val_mae: 0.0536 - learning_rate: 5.0566e-05\n",
            "Epoch 74/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0611 - val_loss: 0.0056 - val_mae: 0.0490 - learning_rate: 5.0566e-05\n",
            "Epoch 75/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0076 - mae: 0.0604 - val_loss: 0.0055 - val_mae: 0.0477 - learning_rate: 5.0566e-05\n",
            "Epoch 76/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 0.0077 - mae: 0.0608 - val_loss: 0.0055 - val_mae: 0.0469 - learning_rate: 5.0566e-05\n",
            "Epoch 77/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0078 - mae: 0.0619 - val_loss: 0.0069 - val_mae: 0.0581 - learning_rate: 5.0566e-05\n",
            "Epoch 78/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0077 - mae: 0.0610 - val_loss: 0.0062 - val_mae: 0.0529 - learning_rate: 5.0566e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0077 - mae: 0.0611 - val_loss: 0.0062 - val_mae: 0.0536 - learning_rate: 5.0566e-05\n",
            "Epoch 80/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 10ms/step - loss: 0.0074 - mae: 0.0599 - val_loss: 0.0053 - val_mae: 0.0468 - learning_rate: 5.0566e-05\n",
            "Epoch 81/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0590 - val_loss: 0.0070 - val_mae: 0.0586 - learning_rate: 5.0566e-05\n",
            "Epoch 82/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0587 - val_loss: 0.0066 - val_mae: 0.0564 - learning_rate: 5.0566e-05\n",
            "Epoch 83/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0594 - val_loss: 0.0076 - val_mae: 0.0629 - learning_rate: 5.0566e-05\n",
            "Epoch 84/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0081 - mae: 0.0631 - val_loss: 0.0058 - val_mae: 0.0500 - learning_rate: 5.0566e-05\n",
            "Epoch 85/100\n",
            "\u001b[1m652/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - mae: 0.0591\n",
            "Epoch 85: ReduceLROnPlateau reducing learning rate to 2.5283030481659807e-05.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - loss: 0.0073 - mae: 0.0591 - val_loss: 0.0057 - val_mae: 0.0506 - learning_rate: 5.0566e-05\n",
            "Epoch 86/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - loss: 0.0072 - mae: 0.0588 - val_loss: 0.0056 - val_mae: 0.0490 - learning_rate: 2.5283e-05\n",
            "Epoch 87/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0595 - val_loss: 0.0055 - val_mae: 0.0494 - learning_rate: 2.5283e-05\n",
            "Epoch 88/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0072 - mae: 0.0594 - val_loss: 0.0059 - val_mae: 0.0527 - learning_rate: 2.5283e-05\n",
            "Epoch 89/100\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0583 - val_loss: 0.0061 - val_mae: 0.0535 - learning_rate: 2.5283e-05\n",
            "Epoch 90/100\n",
            "\u001b[1m654/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0068 - mae: 0.0566\n",
            "Epoch 90: ReduceLROnPlateau reducing learning rate to 1.2641515240829904e-05.\n",
            "\u001b[1m656/656\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.0068 - mae: 0.0566 - val_loss: 0.0055 - val_mae: 0.0487 - learning_rate: 2.5283e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m164/164\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Advanced Optimized DNN (with log transformation) Validation MAE: 2282725.2375\n",
            "Advanced Optimized DNN (with log transformation) Validation RMSE: 2964604.2458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1. Load data\n",
        "# ----------------------------\n",
        "df = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Split train/val\n",
        "# ----------------------------\n",
        "X = df.drop(columns=['force_meas'])\n",
        "y = df['force_meas'].values\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare test\n",
        "X_test = df_test.drop(columns=['force_meas','force_pre'])\n",
        "y_test = df_test['force_meas'].values\n",
        "baseline_pre = df_test['force_pre'].values\n",
        "\n",
        "# ----------------------------\n",
        "# 3. One-hot encode\n",
        "# ----------------------------\n",
        "def onehot(df_in):\n",
        "    return pd.get_dummies(df_in, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "X_train = onehot(X_train)\n",
        "X_val   = onehot(X_val)\n",
        "X_test  = onehot(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Align columns\n",
        "# ----------------------------\n",
        "cols = X_train.columns.tolist()\n",
        "with open('training_columns.pkl','wb') as f:\n",
        "    pickle.dump(cols, f)\n",
        "\n",
        "X_val   = X_val.reindex(columns=cols, fill_value=0)\n",
        "X_test  = X_test.reindex(columns=cols, fill_value=0)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Scale features\n",
        "# ----------------------------\n",
        "scaler = StandardScaler().fit(X_train.values)\n",
        "X_train_s = scaler.transform(X_train.values)\n",
        "X_val_s   = scaler.transform(X_val.values)\n",
        "X_test_s  = scaler.transform(X_test.values)\n",
        "len(X_test_s)"
      ],
      "metadata": {
        "id": "a2_dyMA1UMpx",
        "outputId": "03a01440-457e-4b25-93ef-d184da6c9ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "1) Load train/test\n",
        "2) Preprocess (one-hot, align columns, scale)\n",
        "3) Log-transform y\n",
        "4) Tune + train on scaled data\n",
        "5) Final evaluate on test.csv\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import keras_tuner as kt\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load data\n",
        "# ----------------------------\n",
        "df = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Split train/val\n",
        "# ----------------------------\n",
        "X = df.drop(columns=['force_meas'])\n",
        "y = df['force_meas'].values\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare test\n",
        "X_test = df_test.drop(columns=['force_meas','force_pre'])\n",
        "y_test = df_test['force_meas'].values\n",
        "baseline_pre = df_test['force_pre'].values\n",
        "\n",
        "# ----------------------------\n",
        "# 3. One-hot encode\n",
        "# ----------------------------\n",
        "def onehot(df_in):\n",
        "    return pd.get_dummies(df_in, columns=['tappingsteelgrade'], drop_first=True)\n",
        "\n",
        "X_train = onehot(X_train)\n",
        "X_val   = onehot(X_val)\n",
        "X_test  = onehot(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Align columns\n",
        "# ----------------------------\n",
        "cols = X_train.columns.tolist()\n",
        "with open('training_columns.pkl','wb') as f:\n",
        "    pickle.dump(cols, f)\n",
        "\n",
        "X_val   = X_val.reindex(columns=cols, fill_value=0)\n",
        "X_test  = X_test.reindex(columns=cols, fill_value=0)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Scale features\n",
        "# ----------------------------\n",
        "scaler = StandardScaler().fit(X_train.values)\n",
        "X_train_s = scaler.transform(X_train.values)\n",
        "X_val_s   = scaler.transform(X_val.values)\n",
        "X_test_s  = scaler.transform(X_test.values)\n",
        "\n",
        "with open('scaler_dnn_log.pkl','wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Log-transform y\n",
        "# ----------------------------\n",
        "y_train_log = np.log(y_train)\n",
        "y_val_log   = np.log(y_val)\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Build tuner\n",
        "# ----------------------------\n",
        "input_dim = X_train_s.shape[1]\n",
        "\n",
        "def build_model(hp):\n",
        "    m = keras.Sequential([layers.Input(shape=(input_dim,))])\n",
        "    for i in range(hp.Int('num_layers',4,6)):\n",
        "        units = hp.Int(f'units_{i}',32,512,32)\n",
        "        l2r   = hp.Float(f'l2_{i}',1e-6,1e-3,sampling='LOG')\n",
        "        m.add(layers.Dense(units, kernel_regularizer=regularizers.l2(l2r)))\n",
        "        if hp.Choice(f'act_{i}', ['relu','leaky_relu'])=='leaky_relu':\n",
        "            m.add(layers.LeakyReLU())\n",
        "        else:\n",
        "            m.add(layers.Activation('relu'))\n",
        "        m.add(layers.Dropout(hp.Float(f'drop_{i}',0,0.5,0.1)))\n",
        "        if hp.Boolean(f'bn_{i}'):\n",
        "            m.add(layers.BatchNormalization())\n",
        "    m.add(layers.Dense(1))\n",
        "    lr = hp.Float('lr',1e-4,1e-2,sampling='LOG')\n",
        "    m.compile(optimizer=keras.optimizers.Adam(lr),\n",
        "              loss='mse', metrics=['mae'])\n",
        "    return m\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model, objective='val_mae',\n",
        "    max_trials=30, executions_per_trial=1,\n",
        "    directory='advanced_dnn_tuning', project_name='rolling_force'\n",
        ")\n",
        "tuner.search(X_train_s, y_train_log,\n",
        "             validation_data=(X_val_s, y_val_log),\n",
        "             epochs=50,\n",
        "             callbacks=[keras.callbacks.EarlyStopping('val_mae',patience=5)])\n",
        "\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"Best HP:\", best_hp.values)\n",
        "\n",
        "# ----------------------------\n",
        "# 8. Train best model\n",
        "# ----------------------------\n",
        "model = tuner.hypermodel.build(best_hp)\n",
        "callbacks = [\n",
        "    keras.callbacks.ReduceLROnPlateau('val_loss',factor=0.5,patience=5,verbose=1),\n",
        "    keras.callbacks.EarlyStopping('val_loss',patience=10,restore_best_weights=True)\n",
        "]\n",
        "model.fit(X_train_s, y_train_log,\n",
        "          validation_data=(X_val_s,y_val_log),\n",
        "          epochs=100, batch_size=256,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.save('optimized_dnn_model_advanced_log.h5')\n",
        "\n",
        "# ----------------------------\n",
        "# 9. Final evaluation on test.csv\n",
        "# ----------------------------\n",
        "start = time.time()\n",
        "y_pred_log = model.predict(X_test_s).flatten()\n",
        "runtime = (time.time()-start)/len(X_test_s)\n",
        "\n",
        "y_pred = np.exp(y_pred_log)\n",
        "\n",
        "mae  = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mse  = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"DNN Test MAE: {mae:.4f}, RMSE: {rmse:.4f}, MSE: {mse:.4f}, time/sample: {runtime:.6f}s\")\n",
        "\n",
        "# baseline\n",
        "b_mae  = mean_absolute_error(y_test, baseline_pre)\n",
        "b_rmse = np.sqrt(mean_squared_error(y_test, baseline_pre))\n",
        "b_mse  = mean_squared_error(y_test, baseline_pre)\n",
        "print(f\"Baseline MAE: {b_mae:.4f}, RMSE: {b_rmse:.4f}, MSE: {b_mse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mA5qveYVr6z",
        "outputId": "2d21b067-b1ef-4451-c4f5-088281faab40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 02m 17s]\n",
            "val_mae: 0.14077308773994446\n",
            "\n",
            "Best val_mae So Far: 0.06351888924837112\n",
            "Total elapsed time: 03h 59m 07s\n",
            "Best HP: {'num_layers': 6, 'units_0': 320, 'l2_0': 8.140477497731987e-06, 'act_0': 'leaky_relu', 'drop_0': 0.0, 'bn_0': True, 'units_1': 192, 'l2_1': 0.00016867868524984378, 'act_1': 'relu', 'drop_1': 0.0, 'bn_1': False, 'units_2': 64, 'l2_2': 3.2728612900580493e-06, 'act_2': 'relu', 'drop_2': 0.30000000000000004, 'bn_2': False, 'units_3': 160, 'l2_3': 5.23637387408626e-06, 'act_3': 'leaky_relu', 'drop_3': 0.0, 'bn_3': True, 'lr': 0.0011116670239494002, 'units_4': 128, 'l2_4': 1.521896631722413e-05, 'act_4': 'relu', 'drop_4': 0.4, 'bn_4': True, 'units_5': 96, 'l2_5': 0.00010541710757601335, 'act_5': 'leaky_relu', 'drop_5': 0.4, 'bn_5': True}\n",
            "Epoch 1/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - loss: 268.9745 - mae: 16.2326 - val_loss: 61.0838 - val_mae: 7.7847 - learning_rate: 0.0011\n",
            "Epoch 2/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 31.4442 - mae: 4.8032 - val_loss: 0.4228 - val_mae: 0.4751 - learning_rate: 0.0011\n",
            "Epoch 3/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 4.8632 - mae: 1.3170 - val_loss: 0.2909 - val_mae: 0.3326 - learning_rate: 0.0011\n",
            "Epoch 4/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 3.1010 - mae: 1.0120 - val_loss: 0.1306 - val_mae: 0.2101 - learning_rate: 0.0011\n",
            "Epoch 5/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 2.1914 - mae: 0.8265 - val_loss: 0.1252 - val_mae: 0.2024 - learning_rate: 0.0011\n",
            "Epoch 6/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 1.4735 - mae: 0.6757 - val_loss: 0.1052 - val_mae: 0.1719 - learning_rate: 0.0011\n",
            "Epoch 7/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - loss: 1.0640 - mae: 0.5628 - val_loss: 0.1013 - val_mae: 0.1660 - learning_rate: 0.0011\n",
            "Epoch 8/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.7232 - mae: 0.4635 - val_loss: 0.0985 - val_mae: 0.1626 - learning_rate: 0.0011\n",
            "Epoch 9/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.5122 - mae: 0.3909 - val_loss: 0.0929 - val_mae: 0.1548 - learning_rate: 0.0011\n",
            "Epoch 10/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.3474 - mae: 0.3236 - val_loss: 0.0898 - val_mae: 0.1510 - learning_rate: 0.0011\n",
            "Epoch 11/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.2636 - mae: 0.2782 - val_loss: 0.0876 - val_mae: 0.1497 - learning_rate: 0.0011\n",
            "Epoch 12/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.1886 - mae: 0.2368 - val_loss: 0.0855 - val_mae: 0.1486 - learning_rate: 0.0011\n",
            "Epoch 13/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.1440 - mae: 0.2089 - val_loss: 0.0828 - val_mae: 0.1465 - learning_rate: 0.0011\n",
            "Epoch 14/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1170 - mae: 0.1865 - val_loss: 0.0805 - val_mae: 0.1456 - learning_rate: 0.0011\n",
            "Epoch 15/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.1000 - mae: 0.1722 - val_loss: 0.0777 - val_mae: 0.1431 - learning_rate: 0.0011\n",
            "Epoch 16/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0887 - mae: 0.1606 - val_loss: 0.0750 - val_mae: 0.1418 - learning_rate: 0.0011\n",
            "Epoch 17/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0820 - mae: 0.1548 - val_loss: 0.0722 - val_mae: 0.1397 - learning_rate: 0.0011\n",
            "Epoch 18/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0761 - mae: 0.1482 - val_loss: 0.0691 - val_mae: 0.1372 - learning_rate: 0.0011\n",
            "Epoch 19/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0719 - mae: 0.1434 - val_loss: 0.0664 - val_mae: 0.1357 - learning_rate: 0.0011\n",
            "Epoch 20/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0684 - mae: 0.1408 - val_loss: 0.0629 - val_mae: 0.1325 - learning_rate: 0.0011\n",
            "Epoch 21/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0647 - mae: 0.1369 - val_loss: 0.0597 - val_mae: 0.1293 - learning_rate: 0.0011\n",
            "Epoch 22/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0612 - mae: 0.1339 - val_loss: 0.0569 - val_mae: 0.1276 - learning_rate: 0.0011\n",
            "Epoch 23/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0581 - mae: 0.1309 - val_loss: 0.0539 - val_mae: 0.1249 - learning_rate: 0.0011\n",
            "Epoch 24/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0543 - mae: 0.1267 - val_loss: 0.0506 - val_mae: 0.1217 - learning_rate: 0.0011\n",
            "Epoch 25/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0500 - mae: 0.1199 - val_loss: 0.0449 - val_mae: 0.1103 - learning_rate: 0.0011\n",
            "Epoch 26/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0447 - mae: 0.1087 - val_loss: 0.0401 - val_mae: 0.0993 - learning_rate: 0.0011\n",
            "Epoch 27/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0396 - mae: 0.0983 - val_loss: 0.0354 - val_mae: 0.0876 - learning_rate: 0.0011\n",
            "Epoch 28/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0358 - mae: 0.0897 - val_loss: 0.0336 - val_mae: 0.0876 - learning_rate: 0.0011\n",
            "Epoch 29/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0335 - mae: 0.0863 - val_loss: 0.0295 - val_mae: 0.0750 - learning_rate: 0.0011\n",
            "Epoch 30/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0317 - mae: 0.0845 - val_loss: 0.0285 - val_mae: 0.0774 - learning_rate: 0.0011\n",
            "Epoch 31/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0297 - mae: 0.0804 - val_loss: 0.0256 - val_mae: 0.0671 - learning_rate: 0.0011\n",
            "Epoch 32/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0286 - mae: 0.0796 - val_loss: 0.0261 - val_mae: 0.0733 - learning_rate: 0.0011\n",
            "Epoch 33/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0281 - mae: 0.0806 - val_loss: 0.0276 - val_mae: 0.0826 - learning_rate: 0.0011\n",
            "Epoch 34/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - loss: 0.0269 - mae: 0.0785 - val_loss: 0.0261 - val_mae: 0.0792 - learning_rate: 0.0011\n",
            "Epoch 35/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0254 - mae: 0.0751 - val_loss: 0.0248 - val_mae: 0.0755 - learning_rate: 0.0011\n",
            "Epoch 36/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - loss: 0.0246 - mae: 0.0738 - val_loss: 0.0232 - val_mae: 0.0727 - learning_rate: 0.0011\n",
            "Epoch 37/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0243 - mae: 0.0742 - val_loss: 0.0247 - val_mae: 0.0808 - learning_rate: 0.0011\n",
            "Epoch 38/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0238 - mae: 0.0745 - val_loss: 0.0258 - val_mae: 0.0884 - learning_rate: 0.0011\n",
            "Epoch 39/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0232 - mae: 0.0741 - val_loss: 0.0217 - val_mae: 0.0707 - learning_rate: 0.0011\n",
            "Epoch 40/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0227 - mae: 0.0730 - val_loss: 0.0226 - val_mae: 0.0775 - learning_rate: 0.0011\n",
            "Epoch 41/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0216 - mae: 0.0714 - val_loss: 0.0239 - val_mae: 0.0845 - learning_rate: 0.0011\n",
            "Epoch 42/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0215 - mae: 0.0729 - val_loss: 0.0217 - val_mae: 0.0784 - learning_rate: 0.0011\n",
            "Epoch 43/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0209 - mae: 0.0723 - val_loss: 0.0211 - val_mae: 0.0781 - learning_rate: 0.0011\n",
            "Epoch 44/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0202 - mae: 0.0717 - val_loss: 0.0228 - val_mae: 0.0872 - learning_rate: 0.0011\n",
            "Epoch 45/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0195 - mae: 0.0711 - val_loss: 0.0214 - val_mae: 0.0803 - learning_rate: 0.0011\n",
            "Epoch 46/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0194 - mae: 0.0725 - val_loss: 0.0192 - val_mae: 0.0743 - learning_rate: 0.0011\n",
            "Epoch 47/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0179 - mae: 0.0686 - val_loss: 0.0208 - val_mae: 0.0828 - learning_rate: 0.0011\n",
            "Epoch 48/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0171 - mae: 0.0677 - val_loss: 0.0226 - val_mae: 0.0967 - learning_rate: 0.0011\n",
            "Epoch 49/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0169 - mae: 0.0693 - val_loss: 0.0247 - val_mae: 0.1046 - learning_rate: 0.0011\n",
            "Epoch 50/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0164 - mae: 0.0696 - val_loss: 0.0196 - val_mae: 0.0870 - learning_rate: 0.0011\n",
            "Epoch 51/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0152 - mae: 0.0672 - val_loss: 0.0174 - val_mae: 0.0788 - learning_rate: 0.0011\n",
            "Epoch 52/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0145 - mae: 0.0665 - val_loss: 0.0156 - val_mae: 0.0735 - learning_rate: 0.0011\n",
            "Epoch 53/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0145 - mae: 0.0683 - val_loss: 0.0168 - val_mae: 0.0832 - learning_rate: 0.0011\n",
            "Epoch 54/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0136 - mae: 0.0680 - val_loss: 0.0210 - val_mae: 0.0986 - learning_rate: 0.0011\n",
            "Epoch 55/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0137 - mae: 0.0692 - val_loss: 0.0153 - val_mae: 0.0814 - learning_rate: 0.0011\n",
            "Epoch 56/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0124 - mae: 0.0655 - val_loss: 0.0159 - val_mae: 0.0842 - learning_rate: 0.0011\n",
            "Epoch 57/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0128 - mae: 0.0701 - val_loss: 0.0154 - val_mae: 0.0834 - learning_rate: 0.0011\n",
            "Epoch 58/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0112 - mae: 0.0644 - val_loss: 0.0190 - val_mae: 0.0991 - learning_rate: 0.0011\n",
            "Epoch 59/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0112 - mae: 0.0664 - val_loss: 0.0149 - val_mae: 0.0849 - learning_rate: 0.0011\n",
            "Epoch 60/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 0.0106 - mae: 0.0657 - val_loss: 0.0127 - val_mae: 0.0769 - learning_rate: 0.0011\n",
            "Epoch 61/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0102 - mae: 0.0642 - val_loss: 0.0170 - val_mae: 0.0939 - learning_rate: 0.0011\n",
            "Epoch 62/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0104 - mae: 0.0668 - val_loss: 0.0097 - val_mae: 0.0664 - learning_rate: 0.0011\n",
            "Epoch 63/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0095 - mae: 0.0636 - val_loss: 0.0113 - val_mae: 0.0739 - learning_rate: 0.0011\n",
            "Epoch 64/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0093 - mae: 0.0634 - val_loss: 0.0111 - val_mae: 0.0731 - learning_rate: 0.0011\n",
            "Epoch 65/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0098 - mae: 0.0671 - val_loss: 0.0176 - val_mae: 0.0999 - learning_rate: 0.0011\n",
            "Epoch 66/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0103 - mae: 0.0696 - val_loss: 0.0086 - val_mae: 0.0628 - learning_rate: 0.0011\n",
            "Epoch 67/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0087 - mae: 0.0628 - val_loss: 0.0097 - val_mae: 0.0687 - learning_rate: 0.0011\n",
            "Epoch 68/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0086 - mae: 0.0628 - val_loss: 0.0141 - val_mae: 0.0874 - learning_rate: 0.0011\n",
            "Epoch 69/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - loss: 0.0089 - mae: 0.0627 - val_loss: 0.0189 - val_mae: 0.0826 - learning_rate: 0.0011\n",
            "Epoch 70/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0089 - mae: 0.0642 - val_loss: 0.0086 - val_mae: 0.0654 - learning_rate: 0.0011\n",
            "Epoch 71/100\n",
            "\u001b[1m196/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0084 - mae: 0.0628\n",
            "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.0005558335105888546.\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0084 - mae: 0.0628 - val_loss: 0.0104 - val_mae: 0.0742 - learning_rate: 0.0011\n",
            "Epoch 72/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0073 - mae: 0.0572 - val_loss: 0.0114 - val_mae: 0.0785 - learning_rate: 5.5583e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0068 - mae: 0.0561 - val_loss: 0.0099 - val_mae: 0.0729 - learning_rate: 5.5583e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0070 - mae: 0.0574 - val_loss: 0.0073 - val_mae: 0.0599 - learning_rate: 5.5583e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0069 - mae: 0.0568 - val_loss: 0.0075 - val_mae: 0.0612 - learning_rate: 5.5583e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - loss: 0.0069 - mae: 0.0569 - val_loss: 0.0115 - val_mae: 0.0804 - learning_rate: 5.5583e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0068 - mae: 0.0567 - val_loss: 0.0091 - val_mae: 0.0676 - learning_rate: 5.5583e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 0.0068 - mae: 0.0573 - val_loss: 0.0066 - val_mae: 0.0555 - learning_rate: 5.5583e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0069 - mae: 0.0569 - val_loss: 0.0099 - val_mae: 0.0737 - learning_rate: 5.5583e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0066 - mae: 0.0562 - val_loss: 0.0104 - val_mae: 0.0755 - learning_rate: 5.5583e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0065 - mae: 0.0556 - val_loss: 0.0086 - val_mae: 0.0672 - learning_rate: 5.5583e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - loss: 0.0067 - mae: 0.0567 - val_loss: 0.0079 - val_mae: 0.0594 - learning_rate: 5.5583e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0070 - mae: 0.0586\n",
            "Epoch 83: ReduceLROnPlateau reducing learning rate to 0.0002779167552944273.\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0070 - mae: 0.0586 - val_loss: 0.0076 - val_mae: 0.0622 - learning_rate: 5.5583e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 0.0061 - mae: 0.0534 - val_loss: 0.0091 - val_mae: 0.0691 - learning_rate: 2.7792e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0057 - mae: 0.0518 - val_loss: 0.0078 - val_mae: 0.0645 - learning_rate: 2.7792e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 0.0057 - mae: 0.0523 - val_loss: 0.0092 - val_mae: 0.0722 - learning_rate: 2.7792e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - loss: 0.0058 - mae: 0.0531 - val_loss: 0.0075 - val_mae: 0.0628 - learning_rate: 2.7792e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m198/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0057 - mae: 0.0523\n",
            "Epoch 88: ReduceLROnPlateau reducing learning rate to 0.00013895837764721364.\n",
            "\u001b[1m200/200\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 0.0057 - mae: 0.0523 - val_loss: 0.0075 - val_mae: 0.0640 - learning_rate: 2.7792e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "DNN Test MAE: 2652448.4115, RMSE: 3442774.0734, MSE: 11852693320321.5332, time/sample: 0.000085s\n",
            "Baseline MAE: 2898684.1133, RMSE: 3775600.3207, MSE: 14255157781834.8516\n"
          ]
        }
      ]
    }
  ]
}